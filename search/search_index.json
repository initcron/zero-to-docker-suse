{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Zero to Docker Welcome to Zero to Docker program by School of Devops This is a Lab Guide which goes along with this Docker course by School of Devops. For information about the devops trainign courses visit schoolofdevops.com . Team Gourav Shah Vijayboopathy","title":"Home"},{"location":"#zero-to-docker","text":"Welcome to Zero to Docker program by School of Devops This is a Lab Guide which goes along with this Docker course by School of Devops. For information about the devops trainign courses visit schoolofdevops.com .","title":"Zero to Docker"},{"location":"#team","text":"Gourav Shah Vijayboopathy","title":"Team"},{"location":"Docker Registry Setup/","text":"Docker Registry The Registry is a stateless, highly scalable server side application that stores and lets you distribute Docker images. Setting up the Docker Registry docker run -d -p 5000:5000 --name registry registry:2 This pulls the registry:2 image from the docker hub and runs the docker container. The name of the container is Registry. docker pull ubuntu && docker tag ubuntu localhost:5000/myfirstimage This pulls the ubuntu image from the docker hub. And we tag the image to localhost:5000/myfirstimage docker push localhost:5000/myfirstimage When we try to push the image, it locally saves in the host file system. This we call as the Docker Registry To stop the Registry we use docker stop registry To remove Registry with all the data docker rm -v registry Docker Private Registry Docker Private Registry is to deploy the docker images present in one VM or host to another VM or Host. Create a file system like given below in the host computer . \u251c\u2500\u2500 Docker \u2502 \u2514\u2500\u2500 Vagrantfile \u2514\u2500\u2500 Docker-Client \u2514\u2500\u2500 Vagrantfile Both vagrant file bringing up the same Vagrant Box file. Change the ip in the both the Vagrentfile as \"192.168.33.10\" and \"192.168.33.11\" Now bring up the VM in the Docker directory. vagrant up; vagrant ssh Once both the machines are up add an entry in /etc/hosts <Machine IP> myregistrydomain.com Note: Replace <Machine IP> with the respective vagrant machine IP's We need to install Docker Compose first. yum install python-pip pip install docker-compose Setting Docker Registry After isntalling Docker Compose, its time to setup the .yml file for the Docker Compose. Since we need a place to store the list of users who can access our Registry we need a place to store it. So we are going to install httpd-tools which contains the package called as htpasswd yum install httpd-tools mkdir ~/docker-registry && cd $_ mkdir data In the docker-registry, we create the .yml file for compose. vim docker-compose.yml Now add the following contents contents registry: image: registry:2 ports: - 127.0.0.1:5000:5000 environment: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY: /data volumes: - ./data:/data Now save the file. And run docker compose. docker-compose up -d Now to stop and remove the container that have been created docker-compose stop docker-compose rm Setting Nginx container Edit docker compose file so that we can add the configurations for Nginx. vim docker-compose.yml Add the contents nginx: image: \"nginx:1.9\" ports: - 5043:443 links: - registry:registry volumes: - ./nginx/:/etc/nginx/conf.d:ro The full docker-compose.yml looks like nginx: image: \"nginx:1.9\" ports: - 5043:443 links: - registry:registry volumes: - ./nginx/:/etc/nginx/conf.d registry: image: registry:2 ports: - 127.0.0.1:5000:5000 environment: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY: /data volumes: - ./data:/data Now we going to edit the registry configurations for the SSL certification mkdir -p ~/docker-registry/nginx/ vim ~/docker-registry/nginx/registry.conf Add the following contents upstream docker-registry { server registry:5000; } server { listen 443; server_name myregistrydomain.com; # SSL # ssl on; # ssl_certificate /etc/nginx/conf.d/domain.crt; # ssl_certificate_key /etc/nginx/conf.d/domain.key; # disable any limits to avoid HTTP 413 for large image uploads client_max_body_size 0; # required to avoid HTTP 411: see Issue #1486 (https://github.com/docker/docker/issues/1486) chunked_transfer_encoding on; location /v2/ { # Do not allow connections from docker 1.5 and earlier # docker pre-1.6.0 did not properly set the user agent on ping, catch \"Go *\" user agents if ($http_user_agent ~ \"^(docker\\/1\\.(3|4|5(?!\\.[0-9]-dev))|Go ).*$\" ) { return 404; } # To add basic authentication to v2 use auth_basic setting plus add_header # auth_basic \"registry.localhost\"; # auth_basic_user_file /etc/nginx/conf.d/registry.password; # add_header 'Docker-Distribution-Api-Version' 'registry/2.0' always; proxy_pass http://docker-registry; proxy_set_header Host $http_host; # required for docker client's sake proxy_set_header X-Real-IP $remote_addr; # pass on real client's IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_read_timeout 900; } } Save the file and exit Now again start the docker compose. docker-compose up -d To check whether Docker Registry is working or not try the following command curl http://localhost:5000/v2/ The output will be {} And also try curl http://localhost:5043/v2/ The output will be {} Now to stop and remove the container that have been created docker-compose stop docker-compose rm Next we going to create user and their password who can login and access our Docker Registry (Replace USERNAME with your username ) cd ~/docker-registry/nginx htpasswd -c registry.password USERNAME For the first time when we run the command we use the option -c so that it creates the file. After that we dont need to use that option when we create Username Now in the registry.conf which we created we are going to make some changes vim ~/docker-registry/nginx/registry.conf Delete the below lines and # To add basic authentication to v2 use auth_basic setting plus add_header # auth_basic \"registry.localhost\"; # auth_basic_user_file /etc/nginx/conf.d/registry.password; # add_header 'Docker-Distribution-Api-Version' 'registry/2.0' always; And add the following lines # To add basic authentication to v2 use auth_basic setting plus add_header auth_basic \"registry.localhost\"; auth_basic_user_file /etc/nginx/conf.d/registry.password; add_header 'Docker-Distribution-Api-Version' 'registry/2.0' always; Now we are going to start the Docker Compose to see the changes that we have done in the configuration file. cd ~/docker-registry docker-compose up -d Now check the in the command line curl http://localhost:5043/v2/ [Output] <html> <head><title>401 Authorization Required</title></head> <body bgcolor=\"white\"> <center><h1>401 Authorization Required</h1></center> <hr><center>nginx/1.9.7</center> </body> </html> This is because we we have given the registry.password file for the authentication. Now try the below command to check curl http://USERNAME:PASSWORD@localhost:5043/v2/ Now to stop and remove the container that have been created docker-compose stop docker-compose rm Setitng the ssl certificates For SSL certificate go to the registry.conf files and do the following changes nano ~/docker-registry/nginx/registry.conf Delete the following lines of code server { listen 443; server_name myregistrydomain.com; # SSL # ssl on; # ssl_certificate /etc/nginx/conf.d/domain.crt; # ssl_certificate_key /etc/nginx/conf.d/domain.key; And add these lines # SSL ssl on; ssl_certificate /etc/nginx/conf.d/domain.crt; ssl_certificate_key /etc/nginx/conf.d/domain.key; Save and exit the file. Creating and Signing the Certificates Now we are going to create and sign the SSL certificate. cd ~/docker-registry/nginx Generating the root key: openssl genrsa -out devdockerCA.key 2048 Generating the root certificate(Press Enter whenever it prompts): openssl req -x509 -new -nodes -key devdockerCA.key -days 10000 -out devdockerCA.crt Generating the key for the server: openssl genrsa -out domain.key 2048 Now we are going to send a certificate signing request. Press Enter whenever it prompts and When it asks for the \"Common Name (eg, your name or your server's hostname) []:\" type the ip of the machine or the server address (Which you have given in registry.conf near server_name) Don't create a challenging password openssl req -new -key domain.key -out dev-docker-registry.com.csr Now we are going to sign the certificates request openssl x509 -req -in dev-docker-registry.com.csr -CA devdockerCA.crt -CAkey devdockerCA.key -CAcreateserial -out domain.crt -days 10000 Since the certificates are signed by ourself not by the Certificate Authority we have to tell the docker client that we have the authorized certificates. Now we are going to do that locally by running the following commands, update-ca-trust force-enable cp devdockerCA.crt /etc/pki/ca-trust/source/anchors/ update-ca-trust extract Restart the docker service and run the docker compose file. sudo service docker restart cd ~/docker-registry docker-compose up -d To check the output: curl https://USERNAME:PASSWORD@[YOUR-DOMAIN]:5043/v2/ To stop and remove the container that have been created docker-compose stop docker-compose rm Changing the ports of the Nginx from 5043 to 443 in docker-compose.yml Change it from - 5043:443 to - 443:443 Save and exit Run the compose file docker-compose up -d To check the output: curl https://<YOURUSERNAME>:<YOURPASSWORD>@YOUR-DOMAIN/v2/ Accessing Your Docker Registry from a Client Machine Now we have to copy the devdockerCA file from the Registry VM to the Client VM so that we can access the Docker Registry. sudo cat /docker-registry/nginx/devdockerCA.crt Copy the above data in the file ON CLIENT MACHINE: Paste the data present in the devdockerCA.crt in the below file vim /etc/pki/ca-trust/source/anchors/debdockerCA.crt Now update the certificates and restart the docker service update-ca-trust extract service docker restart Now we can access the Docker Registry that is present in the Other VM by docker login https://YOUR-DOMAIN Username: USERNAME Password: PASSWORD After entering the correct username and their password we must get the following output \"Login Succeeded\" Now lets just pull and run ubuntu images. Do some changes in it and try pushing it to the docker registry. docker run -t -i ubuntu /bin/bash touch initcron exit docker commit $(docker ps -lq) test-image Now lets login to the registry and push the changes that we made in the ubuntu container. docker login https://YOUR-DOMAIN Username: USERNAME Password: PASSWORD docker tag test-image [YOUR-DOMAIN]/test-image docker push [YOUR-DOMAIN]/test-image [Output] latest: digest: sha256:5ea1cfb425544011a3198757f9c6b283fa209a928caabe56063f85f3402363b4 size: 8008 Now check it we can go back to the server and follow the steps docker pull [YOUR-DOMAIN]/test-images docker run -t -i [YOUR-DOMAIN]/test-images /bin/bash ls Now here we can see that the file which we created \"initcron\" is available. The Docker Private Registry is successfully configured.","title":"Docker Registry"},{"location":"Docker Registry Setup/#docker-registry","text":"The Registry is a stateless, highly scalable server side application that stores and lets you distribute Docker images.","title":"Docker Registry"},{"location":"Docker Registry Setup/#setting-up-the-docker-registry","text":"docker run -d -p 5000:5000 --name registry registry:2 This pulls the registry:2 image from the docker hub and runs the docker container. The name of the container is Registry. docker pull ubuntu && docker tag ubuntu localhost:5000/myfirstimage This pulls the ubuntu image from the docker hub. And we tag the image to localhost:5000/myfirstimage docker push localhost:5000/myfirstimage When we try to push the image, it locally saves in the host file system. This we call as the Docker Registry To stop the Registry we use docker stop registry To remove Registry with all the data docker rm -v registry","title":"Setting up the Docker Registry"},{"location":"Docker Registry Setup/#docker-private-registry","text":"Docker Private Registry is to deploy the docker images present in one VM or host to another VM or Host. Create a file system like given below in the host computer . \u251c\u2500\u2500 Docker \u2502 \u2514\u2500\u2500 Vagrantfile \u2514\u2500\u2500 Docker-Client \u2514\u2500\u2500 Vagrantfile Both vagrant file bringing up the same Vagrant Box file. Change the ip in the both the Vagrentfile as \"192.168.33.10\" and \"192.168.33.11\" Now bring up the VM in the Docker directory. vagrant up; vagrant ssh Once both the machines are up add an entry in /etc/hosts <Machine IP> myregistrydomain.com Note: Replace <Machine IP> with the respective vagrant machine IP's We need to install Docker Compose first. yum install python-pip pip install docker-compose","title":"Docker Private Registry"},{"location":"Docker Registry Setup/#setting-docker-registry","text":"After isntalling Docker Compose, its time to setup the .yml file for the Docker Compose. Since we need a place to store the list of users who can access our Registry we need a place to store it. So we are going to install httpd-tools which contains the package called as htpasswd yum install httpd-tools mkdir ~/docker-registry && cd $_ mkdir data In the docker-registry, we create the .yml file for compose. vim docker-compose.yml Now add the following contents contents registry: image: registry:2 ports: - 127.0.0.1:5000:5000 environment: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY: /data volumes: - ./data:/data Now save the file. And run docker compose. docker-compose up -d Now to stop and remove the container that have been created docker-compose stop docker-compose rm","title":"Setting Docker Registry"},{"location":"Docker Registry Setup/#setting-nginx-container","text":"Edit docker compose file so that we can add the configurations for Nginx. vim docker-compose.yml Add the contents nginx: image: \"nginx:1.9\" ports: - 5043:443 links: - registry:registry volumes: - ./nginx/:/etc/nginx/conf.d:ro The full docker-compose.yml looks like nginx: image: \"nginx:1.9\" ports: - 5043:443 links: - registry:registry volumes: - ./nginx/:/etc/nginx/conf.d registry: image: registry:2 ports: - 127.0.0.1:5000:5000 environment: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY: /data volumes: - ./data:/data Now we going to edit the registry configurations for the SSL certification mkdir -p ~/docker-registry/nginx/ vim ~/docker-registry/nginx/registry.conf Add the following contents upstream docker-registry { server registry:5000; } server { listen 443; server_name myregistrydomain.com; # SSL # ssl on; # ssl_certificate /etc/nginx/conf.d/domain.crt; # ssl_certificate_key /etc/nginx/conf.d/domain.key; # disable any limits to avoid HTTP 413 for large image uploads client_max_body_size 0; # required to avoid HTTP 411: see Issue #1486 (https://github.com/docker/docker/issues/1486) chunked_transfer_encoding on; location /v2/ { # Do not allow connections from docker 1.5 and earlier # docker pre-1.6.0 did not properly set the user agent on ping, catch \"Go *\" user agents if ($http_user_agent ~ \"^(docker\\/1\\.(3|4|5(?!\\.[0-9]-dev))|Go ).*$\" ) { return 404; } # To add basic authentication to v2 use auth_basic setting plus add_header # auth_basic \"registry.localhost\"; # auth_basic_user_file /etc/nginx/conf.d/registry.password; # add_header 'Docker-Distribution-Api-Version' 'registry/2.0' always; proxy_pass http://docker-registry; proxy_set_header Host $http_host; # required for docker client's sake proxy_set_header X-Real-IP $remote_addr; # pass on real client's IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_read_timeout 900; } } Save the file and exit Now again start the docker compose. docker-compose up -d To check whether Docker Registry is working or not try the following command curl http://localhost:5000/v2/ The output will be {} And also try curl http://localhost:5043/v2/ The output will be {} Now to stop and remove the container that have been created docker-compose stop docker-compose rm Next we going to create user and their password who can login and access our Docker Registry (Replace USERNAME with your username ) cd ~/docker-registry/nginx htpasswd -c registry.password USERNAME For the first time when we run the command we use the option -c so that it creates the file. After that we dont need to use that option when we create Username Now in the registry.conf which we created we are going to make some changes vim ~/docker-registry/nginx/registry.conf Delete the below lines and # To add basic authentication to v2 use auth_basic setting plus add_header # auth_basic \"registry.localhost\"; # auth_basic_user_file /etc/nginx/conf.d/registry.password; # add_header 'Docker-Distribution-Api-Version' 'registry/2.0' always; And add the following lines # To add basic authentication to v2 use auth_basic setting plus add_header auth_basic \"registry.localhost\"; auth_basic_user_file /etc/nginx/conf.d/registry.password; add_header 'Docker-Distribution-Api-Version' 'registry/2.0' always; Now we are going to start the Docker Compose to see the changes that we have done in the configuration file. cd ~/docker-registry docker-compose up -d Now check the in the command line curl http://localhost:5043/v2/ [Output] <html> <head><title>401 Authorization Required</title></head> <body bgcolor=\"white\"> <center><h1>401 Authorization Required</h1></center> <hr><center>nginx/1.9.7</center> </body> </html> This is because we we have given the registry.password file for the authentication. Now try the below command to check curl http://USERNAME:PASSWORD@localhost:5043/v2/ Now to stop and remove the container that have been created docker-compose stop docker-compose rm","title":"Setting Nginx container"},{"location":"Docker Registry Setup/#setitng-the-ssl-certificates","text":"For SSL certificate go to the registry.conf files and do the following changes nano ~/docker-registry/nginx/registry.conf Delete the following lines of code server { listen 443; server_name myregistrydomain.com; # SSL # ssl on; # ssl_certificate /etc/nginx/conf.d/domain.crt; # ssl_certificate_key /etc/nginx/conf.d/domain.key; And add these lines # SSL ssl on; ssl_certificate /etc/nginx/conf.d/domain.crt; ssl_certificate_key /etc/nginx/conf.d/domain.key; Save and exit the file.","title":"Setitng the ssl certificates"},{"location":"Docker Registry Setup/#creating-and-signing-the-certificates","text":"Now we are going to create and sign the SSL certificate. cd ~/docker-registry/nginx Generating the root key: openssl genrsa -out devdockerCA.key 2048 Generating the root certificate(Press Enter whenever it prompts): openssl req -x509 -new -nodes -key devdockerCA.key -days 10000 -out devdockerCA.crt Generating the key for the server: openssl genrsa -out domain.key 2048 Now we are going to send a certificate signing request. Press Enter whenever it prompts and When it asks for the \"Common Name (eg, your name or your server's hostname) []:\" type the ip of the machine or the server address (Which you have given in registry.conf near server_name) Don't create a challenging password openssl req -new -key domain.key -out dev-docker-registry.com.csr Now we are going to sign the certificates request openssl x509 -req -in dev-docker-registry.com.csr -CA devdockerCA.crt -CAkey devdockerCA.key -CAcreateserial -out domain.crt -days 10000 Since the certificates are signed by ourself not by the Certificate Authority we have to tell the docker client that we have the authorized certificates. Now we are going to do that locally by running the following commands, update-ca-trust force-enable cp devdockerCA.crt /etc/pki/ca-trust/source/anchors/ update-ca-trust extract Restart the docker service and run the docker compose file. sudo service docker restart cd ~/docker-registry docker-compose up -d To check the output: curl https://USERNAME:PASSWORD@[YOUR-DOMAIN]:5043/v2/ To stop and remove the container that have been created docker-compose stop docker-compose rm Changing the ports of the Nginx from 5043 to 443 in docker-compose.yml Change it from - 5043:443 to - 443:443 Save and exit Run the compose file docker-compose up -d To check the output: curl https://<YOURUSERNAME>:<YOURPASSWORD>@YOUR-DOMAIN/v2/","title":"Creating and Signing the Certificates"},{"location":"Docker Registry Setup/#accessing-your-docker-registry-from-a-client-machine","text":"Now we have to copy the devdockerCA file from the Registry VM to the Client VM so that we can access the Docker Registry. sudo cat /docker-registry/nginx/devdockerCA.crt Copy the above data in the file","title":"Accessing Your Docker Registry from a Client Machine"},{"location":"Docker Registry Setup/#on-client-machine","text":"Paste the data present in the devdockerCA.crt in the below file vim /etc/pki/ca-trust/source/anchors/debdockerCA.crt Now update the certificates and restart the docker service update-ca-trust extract service docker restart Now we can access the Docker Registry that is present in the Other VM by docker login https://YOUR-DOMAIN Username: USERNAME Password: PASSWORD After entering the correct username and their password we must get the following output \"Login Succeeded\" Now lets just pull and run ubuntu images. Do some changes in it and try pushing it to the docker registry. docker run -t -i ubuntu /bin/bash touch initcron exit docker commit $(docker ps -lq) test-image Now lets login to the registry and push the changes that we made in the ubuntu container. docker login https://YOUR-DOMAIN Username: USERNAME Password: PASSWORD docker tag test-image [YOUR-DOMAIN]/test-image docker push [YOUR-DOMAIN]/test-image [Output] latest: digest: sha256:5ea1cfb425544011a3198757f9c6b283fa209a928caabe56063f85f3402363b4 size: 8008 Now check it we can go back to the server and follow the steps docker pull [YOUR-DOMAIN]/test-images docker run -t -i [YOUR-DOMAIN]/test-images /bin/bash ls Now here we can see that the file which we created \"initcron\" is available. The Docker Private Registry is successfully configured.","title":"ON CLIENT MACHINE:"},{"location":"chapter1-introduction_to_containers_and_docker/","text":"","title":"Chapter1 introduction to containers and docker"},{"location":"chapter2-microservices_and_docker/","text":"","title":"Chapter2 microservices and docker"},{"location":"chapter3-lerning_environment_setup/","text":"","title":"Chapter3 lerning environment setup"},{"location":"chapter4-getting_started/","text":"Getting Started with Docker In this chapter, we are going to learn about docker shell, the command line utility and how to use it to launch containers. We will also learn what it means to run a container, its lifecycle and perform basic operations such as creating, starting, stopping, removing, pausing containers and checking the status etc. Using docker cli We can use docker cli to interact with docker daemon. Various functions of docker command is given below. Try this yourself by runnig $sudo docker command docker [Output] Usage: docker [OPTIONS] COMMAND [arg...] docker [ --help | -v | --version ] A self-sufficient runtime for containers. Options: --config=~/.docker Location of client config files -D, --debug Enable debug mode -H, --host=[] Daemon socket(s) to connect to -h, --help Print usage -l, --log-level=info Set the logging level --tls Use TLS; implied by --tlsverify --tlscacert=~/.docker/ca.pem Trust certs signed only by this CA --tlscert=~/.docker/cert.pem Path to TLS certificate file --tlskey=~/.docker/key.pem Path to TLS key file --tlsverify Use TLS and verify the remote -v, --version Print version information and quit Commands: attach Attach to a running container build Build an image from a Dockerfile commit Create a new image from a container's changes cp Copy files/folders between a container and the local filesystem create Create a new container diff Inspect changes on a container's filesystem events Get real time events from the server exec Run a command in a running container export Export a container's filesystem as a tar archive history Show the history of an image images List images import Import the contents from a tarball to create a filesystem image info Display system-wide information inspect Return low-level information on a container, image or task kill Kill one or more running containers load Load an image from a tar archive or STDIN login Log in to a Docker registry. logout Log out from a Docker registry. logs Fetch the logs of a container network Manage Docker networks node Manage Docker Swarm nodes pause Pause all processes within one or more containers port List port mappings or a specific mapping for the container ps List containers pull Pull an image or a repository from a registry push Push an image or a repository to a registry rename Rename a container restart Restart a container rm Remove one or more containers rmi Remove one or more images run Run a command in a new container save Save one or more images to a tar archive (streamed to STDOUT by default) search Search the Docker Hub for images service Manage Docker services start Start one or more stopped containers stats Display a live stream of container(s) resource usage statistics stop Stop one or more running containers swarm Manage Docker Swarm tag Tag an image into a repository top Display the running processes of a container unpause Unpause all processes within one or more containers update Update configuration of one or more containers version Show the Docker version information volume Manage Docker volumes wait Block until a container stops, then print its exit code Getting Information about Docker Setup We can get the information about our Docker setup in several ways. Namely, docker -v docker version docker system info [Output of docker -v ] Docker version 18.03.1-ce, build 9ee9f40 [Output of docker version ] Client: Version: 18.03.1-ce API version: 1.37 Go version: go1.9.5 Git commit: 9ee9f40 Built: Thu Apr 26 07:18:46 2018 OS/Arch: linux/amd64 Experimental: false Orchestrator: swarm Server: Engine: Version: 18.03.1-ce API version: 1.37 (minimum version 1.12) Go version: go1.9.5 Git commit: 9ee9f40 Built: Thu Apr 26 07:16:59 2018 OS/Arch: linux/amd64 Experimental: false The docker system info command gives a lot of useful information like total number of containers and images along with information about host resource utilization etc. Launching our first container Now we have a basic understanding of docker command and sub commands, let us dive straight into launching our very first container docker run alpine:3.4 uptime Where, * we are using docker client to * run a application/command uptime using * an image by name alpine:3.4 [Output] Unable to find image 'alpine:3.4' locally 3.4: Pulling from library/alpine 81033e7c1d6a: Pull complete Digest: sha256:2532609239f3a96fbc30670716d87c8861b8a1974564211325633ca093b11c0b Status: Downloaded newer image for alpine:3.4 15:24:34 up 7:36, load average: 0.00, 0.03, 0.04 What happened? This command will * Pull the alpine image file from docker hub , a cloud registry * Create a runtime environment/ container with the above image * Launch a program (called uptime ) inside that container * Stream that output to the terminal * Stop the container once the program is exited Where did my container go? docker container ps docker container ps -l The point here to remember is that, when that executable stops running inside the container, the container itself will stop This process will further be explained under the lifecycle of a container topic. Let's see what happens when we run that command again, [Output] docker run alpine uptime 07:48:06 up 3:15, load average: 0.00, 0.00, 0.00 Now docker no longer pulls the image again from registry, because it has stored the image locally from the previous run So once an image is pulled, we can make use of that image to create and run as many container as we want without the need of downloading the image again. However it has created a new instance of the iamge/container. Checking Status of the containers We have understood how docker run commands works. But what if you want to see list of running containers and history of containers that had run and exited? This can be done by executing the following commands docker ps [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES This command doesn't give us any information. Because, docker ps command will only show list of container(s) which are running docker ps -l [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 988f4d90d604 alpine \"uptime\" About a minute ago Exited (0) About a minute ago fervent_hypatia the -l flag shows the last run container along with other details like image it used, command it executed, return code of that command, etc., docker ps -n 2 [Output] NAMES 988f4d90d604 alpine \"uptime\" About a minute ago Exited (0) About a minute ago fervent_hypatia acea3023dca4 alpine \"uptime\" 3 minutes ago Exited (0) 3 minutes ago mad_darwin Docker gives us the flexibility to show the desirable number of last run containers. This can be achieved by using -n #no_of_results flag docker ps -a [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 988f4d90d604 alpine \"uptime\" About a minute ago Exited (0) About a minute ago fervent_hypatia acea3023dca4 alpine \"uptime\" 4 minutes ago Exited (0) 4 minutes ago mad_darwin 60ffa94e69ec ubuntu:14.04.3 \"bash\" 27 hours ago Exited (0) 26 hours ago infallible_meninsky dd75c04e7d2b schoolofdevops/ghost:0.3.1 \"/entrypoint.sh npm s\" 4 days ago Exited (0) 3 days ago kickass_bardeen c082972f66d6 schoolofdevops/ghost:0.3.1 \"/entrypoint.sh npm s\" 4 days ago Exited (0) 3 days ago 0.0.0.0:80->2368/tcp sodcblog This command will show all the container we have run so far. Running Containers in Interactive Mode We can interact with docker containers by giving -it flags at the run time. These flags stand for * i - Interactive * t - tty docker run -it alpine sh [Output] Unable to find image 'alpine:latest' locally latest: Pulling from library/alpine ff3a5c916c92: Already exists Digest: sha256:7df6db5aa61ae9480f52f0b3a06a140ab98d427f86d8d5de0bedab9b8df6b1c0 Status: Downloaded newer image for alpine:latest / # As you see, we have landed straight into sh shell of that container. This is the result of using -it flags and mentioning that container to run the sh shell. Don't try to exit that container yet. We have to execute some other commands in it to understand the next topic Namespaced: Like a full fledged OS, Docker container has its own namespaces This enables Docker container to isolate itself from the host as well as other containers Run the following commands and see that alpine container has its own namespaces and not inheriting much from host OS [Command] cat /etc/issue [Output] Welcome to Alpine Linux 3.4 Kernel \\r on an \\m (\\l) [Command] ps aux [Output] PID USER TIME COMMAND 1 root 0:00 sh 6 root 0:00 ps aux [Command] ifconfig [Output] eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:acff:fe11:2%32640/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:8 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:648 (648.0 B) TX bytes:648 (648.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1%32640/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) [Command] hostname [Output] ae84d253ecb5 Shared: We have understood that containers have their own namespaces. But will they share something to some extent? the answer is YES . Let's run the following commands on both the container and the host machine [Command] uname -a [Output - container ] Linux ae84d253ecb5 3.10.0-327.28.3.el7.x86_64 #1 SMP Thu Aug 18 19:05:49 UTC 2016 x86_64 Linux [Output - hostmachine ] Linux dockerserver 3.10.0-327.28.3.el7.x86_64 #1 SMP Thu Aug 18 19:05:49 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux As you can see, the container uses the same Linux Kernel from the host machine. Just like uname command, the following commands share the same information as well. In order to avoid repetition, we will see the output of container alone. [Command] date [Output] Wed Sep 14 18:21:25 UTC 2016 [Command] cat /proc/cpuinfo [Output] processor : 0 vendor_id : GenuineIntel cpu family : 6 model : 94 model name : Intel(R) Core(TM) i7-6700HQ CPU @ 2.60GHz stepping : 3 cpu MHz : 2592.002 cache size : 6144 KB physical id : 0 siblings : 1 core id : 0 cpu cores : 1 apicid : 0 initial apicid : 0 fpu : yes fpu_exception : yes cpuid level : 22 wp : yes flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc pni pclmulqdq monitor ssse3 cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx rdrand hypervisor lahf_lm abm 3dnowprefetch rdseed clflushopt bogomips : 5184.00 clflush size : 64 cache_alignment : 64 address sizes : 39 bits physical, 48 bits virtual power management: [Command] free [Output] total used free shared buffers cached Mem: 1884176 650660 1233516 0 1860 473248 -/+ buffers/cache: 175552 1708624 Swap: 1048572 0 1048572 Now exit out of that container by running exit or by pressing ctrl+d Making Containers Persist Running Containers in Detached Mode So far, we have run the containers interactively. But this is not always the case. Sometimes you may want to start a container without interacting with it. This can be achieved by using \"detached mode\" ( -d ) flag. Hence the container will launch the deafault application inside and run in the background. This saves a lot of time, we don't have to wait till the applications launches successfully. It will happen behind the screen. Let us run the following command to see this in action [Command] docker run -idt schoolofdevops/loop program -d , --detach : detached mode [Output] 2533adf280ac4838b446e4ee1151f52793e6ac499d2e631b2c752459bb18ad5f This will run the container in detached mode. We are only given with full container id as the output Let us check whether this container is running or not [Command] docker ps [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2533adf280ac schoolofdevops/loop \"program\" 37 seconds ago Up 36 seconds prickly_bose As we can see in the output, the container is running in the background Checking Logs To check the logs, find out the container id/name and run the following commands, replacing 08f0242aa61c with your container id [Commands] docker container ps docker container logs 08f0242aa61c docker container logs -f 08f0242aa61c Connecting to running container to execute commands We can connect to the containers which are running in detached mode by using these following commands [Command] docker exec -it 2533adf280ac sh [Output] / # You could try running any commands on the shell e.g. apk update apk add vim ps aux Now exit the container. Pausing Running Container Just like in a video, it is easy to pause and unpause the running container [Command] docker pause 2533adf280ac After running pause command, run docker ps again to check the container status [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2533adf280ac schoolofdevops/loop \"program\" 2 minutes ago Up 2 minutes (Paused) prickly_bose Unpausing the paused container This can be achieved by executing following command [Command] docker unpause Run docker ps to verify the changes [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2533adf280ac schoolofdevops/loop \"program\" 6 minutes ago Up 6 minutes prickly_bose Creating and Starting a Container instead of Running docker run command will create a container and start that container simultaneously. However docker gives you the granularity to create a container and not to run it at the time of creation. However, This container can be started by using start command [Command] docker create alpine:3.4 sh Run docker ps -l to see the status of the container [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 22146d15eb71 alpine:3.4 \"sh\" 31 seconds ago Created grave_leavitt If you do docker ps -l , you will find that container status to be Created . Now lets start this container by executing, [Command] docker start 22146d15eb71 Run docker ps -l again to see the status change [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 22146d15eb71 alpine:3.4 \"sh\" 3 minutes ago Exited (0) 2 minutes ago grave_leavitt This command will start the container and exit right away we have not specified interactive mode in the command Creating Pretty Reports with Formatters docker ps --format \"{{.ID}}: {{.Status}}\" [Output] 2533adf280ac: Up 12 minutes Checking disk utilisation by docker system df [output] docker system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 7 5 1.031GB 914.5MB (88%) Containers 8 4 27.97MB 27.73MB (99%) Local Volumes 3 2 0B 0B Build Cache 0B 0B To prune, you could possibly use docker container prune docker system prune e.g. docker system prune WARNING! This will remove: - all stopped containers - all networks not used by at least one container - all dangling images - all build cache Are you sure you want to continue? [y/N] Make sure you understand what all will be removed before using this command. Stopping and Removing Containers We have learnt about interacting with a container, running a container, pausing and unpausing a container, creating and starting a container. But what if you want to stop the container or remove the container itself Stop a container A container can be stopped using stop command. This command will stop the application inside that container hence the container itself will be stopped. This command basically sends a SIGTERM signal to the container (graceful shutdown) [Command] docker stop 2533adf280ac [Output] 2533adf280ac Kill a container This command will send SIGKILL signal and kills the container ungracefully [Command] docker kill 590e7060743a [Output] 590e7060743a If you want to remove a container, then execute the following command. Before running this command, run docker ps -a to see the list of pre run containers. Choose a container of your wish and then execute docker rm command. Then run docker ps -a again to check the removed container list or not [Command] docker rm 590e7060743a [Output] 590e7060743a","title":"Getting Started with Docker"},{"location":"chapter4-getting_started/#getting-started-with-docker","text":"In this chapter, we are going to learn about docker shell, the command line utility and how to use it to launch containers. We will also learn what it means to run a container, its lifecycle and perform basic operations such as creating, starting, stopping, removing, pausing containers and checking the status etc.","title":"Getting Started with Docker"},{"location":"chapter4-getting_started/#using-docker-cli","text":"We can use docker cli to interact with docker daemon. Various functions of docker command is given below. Try this yourself by runnig $sudo docker command docker [Output] Usage: docker [OPTIONS] COMMAND [arg...] docker [ --help | -v | --version ] A self-sufficient runtime for containers. Options: --config=~/.docker Location of client config files -D, --debug Enable debug mode -H, --host=[] Daemon socket(s) to connect to -h, --help Print usage -l, --log-level=info Set the logging level --tls Use TLS; implied by --tlsverify --tlscacert=~/.docker/ca.pem Trust certs signed only by this CA --tlscert=~/.docker/cert.pem Path to TLS certificate file --tlskey=~/.docker/key.pem Path to TLS key file --tlsverify Use TLS and verify the remote -v, --version Print version information and quit Commands: attach Attach to a running container build Build an image from a Dockerfile commit Create a new image from a container's changes cp Copy files/folders between a container and the local filesystem create Create a new container diff Inspect changes on a container's filesystem events Get real time events from the server exec Run a command in a running container export Export a container's filesystem as a tar archive history Show the history of an image images List images import Import the contents from a tarball to create a filesystem image info Display system-wide information inspect Return low-level information on a container, image or task kill Kill one or more running containers load Load an image from a tar archive or STDIN login Log in to a Docker registry. logout Log out from a Docker registry. logs Fetch the logs of a container network Manage Docker networks node Manage Docker Swarm nodes pause Pause all processes within one or more containers port List port mappings or a specific mapping for the container ps List containers pull Pull an image or a repository from a registry push Push an image or a repository to a registry rename Rename a container restart Restart a container rm Remove one or more containers rmi Remove one or more images run Run a command in a new container save Save one or more images to a tar archive (streamed to STDOUT by default) search Search the Docker Hub for images service Manage Docker services start Start one or more stopped containers stats Display a live stream of container(s) resource usage statistics stop Stop one or more running containers swarm Manage Docker Swarm tag Tag an image into a repository top Display the running processes of a container unpause Unpause all processes within one or more containers update Update configuration of one or more containers version Show the Docker version information volume Manage Docker volumes wait Block until a container stops, then print its exit code","title":"Using docker cli"},{"location":"chapter4-getting_started/#getting-information-about-docker-setup","text":"We can get the information about our Docker setup in several ways. Namely, docker -v docker version docker system info [Output of docker -v ] Docker version 18.03.1-ce, build 9ee9f40 [Output of docker version ] Client: Version: 18.03.1-ce API version: 1.37 Go version: go1.9.5 Git commit: 9ee9f40 Built: Thu Apr 26 07:18:46 2018 OS/Arch: linux/amd64 Experimental: false Orchestrator: swarm Server: Engine: Version: 18.03.1-ce API version: 1.37 (minimum version 1.12) Go version: go1.9.5 Git commit: 9ee9f40 Built: Thu Apr 26 07:16:59 2018 OS/Arch: linux/amd64 Experimental: false The docker system info command gives a lot of useful information like total number of containers and images along with information about host resource utilization etc.","title":"Getting Information about Docker Setup"},{"location":"chapter4-getting_started/#launching-our-first-container","text":"Now we have a basic understanding of docker command and sub commands, let us dive straight into launching our very first container docker run alpine:3.4 uptime Where, * we are using docker client to * run a application/command uptime using * an image by name alpine:3.4 [Output] Unable to find image 'alpine:3.4' locally 3.4: Pulling from library/alpine 81033e7c1d6a: Pull complete Digest: sha256:2532609239f3a96fbc30670716d87c8861b8a1974564211325633ca093b11c0b Status: Downloaded newer image for alpine:3.4 15:24:34 up 7:36, load average: 0.00, 0.03, 0.04 What happened? This command will * Pull the alpine image file from docker hub , a cloud registry * Create a runtime environment/ container with the above image * Launch a program (called uptime ) inside that container * Stream that output to the terminal * Stop the container once the program is exited Where did my container go? docker container ps docker container ps -l The point here to remember is that, when that executable stops running inside the container, the container itself will stop This process will further be explained under the lifecycle of a container topic. Let's see what happens when we run that command again, [Output] docker run alpine uptime 07:48:06 up 3:15, load average: 0.00, 0.00, 0.00 Now docker no longer pulls the image again from registry, because it has stored the image locally from the previous run So once an image is pulled, we can make use of that image to create and run as many container as we want without the need of downloading the image again. However it has created a new instance of the iamge/container.","title":"Launching our first container"},{"location":"chapter4-getting_started/#checking-status-of-the-containers","text":"We have understood how docker run commands works. But what if you want to see list of running containers and history of containers that had run and exited? This can be done by executing the following commands docker ps [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES This command doesn't give us any information. Because, docker ps command will only show list of container(s) which are running docker ps -l [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 988f4d90d604 alpine \"uptime\" About a minute ago Exited (0) About a minute ago fervent_hypatia the -l flag shows the last run container along with other details like image it used, command it executed, return code of that command, etc., docker ps -n 2 [Output] NAMES 988f4d90d604 alpine \"uptime\" About a minute ago Exited (0) About a minute ago fervent_hypatia acea3023dca4 alpine \"uptime\" 3 minutes ago Exited (0) 3 minutes ago mad_darwin Docker gives us the flexibility to show the desirable number of last run containers. This can be achieved by using -n #no_of_results flag docker ps -a [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 988f4d90d604 alpine \"uptime\" About a minute ago Exited (0) About a minute ago fervent_hypatia acea3023dca4 alpine \"uptime\" 4 minutes ago Exited (0) 4 minutes ago mad_darwin 60ffa94e69ec ubuntu:14.04.3 \"bash\" 27 hours ago Exited (0) 26 hours ago infallible_meninsky dd75c04e7d2b schoolofdevops/ghost:0.3.1 \"/entrypoint.sh npm s\" 4 days ago Exited (0) 3 days ago kickass_bardeen c082972f66d6 schoolofdevops/ghost:0.3.1 \"/entrypoint.sh npm s\" 4 days ago Exited (0) 3 days ago 0.0.0.0:80->2368/tcp sodcblog This command will show all the container we have run so far.","title":"Checking Status of the containers"},{"location":"chapter4-getting_started/#running-containers-in-interactive-mode","text":"We can interact with docker containers by giving -it flags at the run time. These flags stand for * i - Interactive * t - tty docker run -it alpine sh [Output] Unable to find image 'alpine:latest' locally latest: Pulling from library/alpine ff3a5c916c92: Already exists Digest: sha256:7df6db5aa61ae9480f52f0b3a06a140ab98d427f86d8d5de0bedab9b8df6b1c0 Status: Downloaded newer image for alpine:latest / # As you see, we have landed straight into sh shell of that container. This is the result of using -it flags and mentioning that container to run the sh shell. Don't try to exit that container yet. We have to execute some other commands in it to understand the next topic Namespaced: Like a full fledged OS, Docker container has its own namespaces This enables Docker container to isolate itself from the host as well as other containers Run the following commands and see that alpine container has its own namespaces and not inheriting much from host OS [Command] cat /etc/issue [Output] Welcome to Alpine Linux 3.4 Kernel \\r on an \\m (\\l) [Command] ps aux [Output] PID USER TIME COMMAND 1 root 0:00 sh 6 root 0:00 ps aux [Command] ifconfig [Output] eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:acff:fe11:2%32640/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:8 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:648 (648.0 B) TX bytes:648 (648.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1%32640/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) [Command] hostname [Output] ae84d253ecb5 Shared: We have understood that containers have their own namespaces. But will they share something to some extent? the answer is YES . Let's run the following commands on both the container and the host machine [Command] uname -a [Output - container ] Linux ae84d253ecb5 3.10.0-327.28.3.el7.x86_64 #1 SMP Thu Aug 18 19:05:49 UTC 2016 x86_64 Linux [Output - hostmachine ] Linux dockerserver 3.10.0-327.28.3.el7.x86_64 #1 SMP Thu Aug 18 19:05:49 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux As you can see, the container uses the same Linux Kernel from the host machine. Just like uname command, the following commands share the same information as well. In order to avoid repetition, we will see the output of container alone. [Command] date [Output] Wed Sep 14 18:21:25 UTC 2016 [Command] cat /proc/cpuinfo [Output] processor : 0 vendor_id : GenuineIntel cpu family : 6 model : 94 model name : Intel(R) Core(TM) i7-6700HQ CPU @ 2.60GHz stepping : 3 cpu MHz : 2592.002 cache size : 6144 KB physical id : 0 siblings : 1 core id : 0 cpu cores : 1 apicid : 0 initial apicid : 0 fpu : yes fpu_exception : yes cpuid level : 22 wp : yes flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc pni pclmulqdq monitor ssse3 cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx rdrand hypervisor lahf_lm abm 3dnowprefetch rdseed clflushopt bogomips : 5184.00 clflush size : 64 cache_alignment : 64 address sizes : 39 bits physical, 48 bits virtual power management: [Command] free [Output] total used free shared buffers cached Mem: 1884176 650660 1233516 0 1860 473248 -/+ buffers/cache: 175552 1708624 Swap: 1048572 0 1048572 Now exit out of that container by running exit or by pressing ctrl+d","title":"Running Containers in Interactive Mode"},{"location":"chapter4-getting_started/#making-containers-persist","text":"","title":"Making Containers Persist"},{"location":"chapter4-getting_started/#running-containers-in-detached-mode","text":"So far, we have run the containers interactively. But this is not always the case. Sometimes you may want to start a container without interacting with it. This can be achieved by using \"detached mode\" ( -d ) flag. Hence the container will launch the deafault application inside and run in the background. This saves a lot of time, we don't have to wait till the applications launches successfully. It will happen behind the screen. Let us run the following command to see this in action [Command] docker run -idt schoolofdevops/loop program -d , --detach : detached mode [Output] 2533adf280ac4838b446e4ee1151f52793e6ac499d2e631b2c752459bb18ad5f This will run the container in detached mode. We are only given with full container id as the output Let us check whether this container is running or not [Command] docker ps [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2533adf280ac schoolofdevops/loop \"program\" 37 seconds ago Up 36 seconds prickly_bose As we can see in the output, the container is running in the background","title":"Running Containers in Detached Mode"},{"location":"chapter4-getting_started/#checking-logs","text":"To check the logs, find out the container id/name and run the following commands, replacing 08f0242aa61c with your container id [Commands] docker container ps docker container logs 08f0242aa61c docker container logs -f 08f0242aa61c","title":"Checking Logs"},{"location":"chapter4-getting_started/#connecting-to-running-container-to-execute-commands","text":"We can connect to the containers which are running in detached mode by using these following commands [Command] docker exec -it 2533adf280ac sh [Output] / # You could try running any commands on the shell e.g. apk update apk add vim ps aux Now exit the container.","title":"Connecting to running container to execute commands"},{"location":"chapter4-getting_started/#pausing-running-container","text":"Just like in a video, it is easy to pause and unpause the running container [Command] docker pause 2533adf280ac After running pause command, run docker ps again to check the container status [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2533adf280ac schoolofdevops/loop \"program\" 2 minutes ago Up 2 minutes (Paused) prickly_bose","title":"Pausing Running Container"},{"location":"chapter4-getting_started/#unpausing-the-paused-container","text":"This can be achieved by executing following command [Command] docker unpause Run docker ps to verify the changes [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2533adf280ac schoolofdevops/loop \"program\" 6 minutes ago Up 6 minutes prickly_bose","title":"Unpausing the paused container"},{"location":"chapter4-getting_started/#creating-and-starting-a-container-instead-of-running","text":"docker run command will create a container and start that container simultaneously. However docker gives you the granularity to create a container and not to run it at the time of creation. However, This container can be started by using start command [Command] docker create alpine:3.4 sh Run docker ps -l to see the status of the container [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 22146d15eb71 alpine:3.4 \"sh\" 31 seconds ago Created grave_leavitt If you do docker ps -l , you will find that container status to be Created . Now lets start this container by executing, [Command] docker start 22146d15eb71 Run docker ps -l again to see the status change [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 22146d15eb71 alpine:3.4 \"sh\" 3 minutes ago Exited (0) 2 minutes ago grave_leavitt This command will start the container and exit right away we have not specified interactive mode in the command","title":"Creating and Starting a Container instead of Running"},{"location":"chapter4-getting_started/#creating-pretty-reports-with-formatters","text":"docker ps --format \"{{.ID}}: {{.Status}}\" [Output] 2533adf280ac: Up 12 minutes","title":"Creating Pretty Reports with Formatters"},{"location":"chapter4-getting_started/#checking-disk-utilisation-by","text":"docker system df [output] docker system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 7 5 1.031GB 914.5MB (88%) Containers 8 4 27.97MB 27.73MB (99%) Local Volumes 3 2 0B 0B Build Cache 0B 0B To prune, you could possibly use docker container prune docker system prune e.g. docker system prune WARNING! This will remove: - all stopped containers - all networks not used by at least one container - all dangling images - all build cache Are you sure you want to continue? [y/N] Make sure you understand what all will be removed before using this command.","title":"Checking disk utilisation by"},{"location":"chapter4-getting_started/#stopping-and-removing-containers","text":"We have learnt about interacting with a container, running a container, pausing and unpausing a container, creating and starting a container. But what if you want to stop the container or remove the container itself","title":"Stopping and Removing Containers"},{"location":"chapter4-getting_started/#stop-a-container","text":"A container can be stopped using stop command. This command will stop the application inside that container hence the container itself will be stopped. This command basically sends a SIGTERM signal to the container (graceful shutdown) [Command] docker stop 2533adf280ac [Output] 2533adf280ac","title":"Stop a container"},{"location":"chapter4-getting_started/#kill-a-container","text":"This command will send SIGKILL signal and kills the container ungracefully [Command] docker kill 590e7060743a [Output] 590e7060743a If you want to remove a container, then execute the following command. Before running this command, run docker ps -a to see the list of pre run containers. Choose a container of your wish and then execute docker rm command. Then run docker ps -a again to check the removed container list or not [Command] docker rm 590e7060743a [Output] 590e7060743a","title":"Kill a container"},{"location":"chapter5-container_operations/","text":"Managing Containers - Learning about Common Container Operations In the previous chapter, we have learnt about container lifecycle management including how to create, launch, connect to, stop and remove containers. In this chapter, we are going to learn how to launch a container with a pre built app image and how to access the app with published ports. We will also learn about common container operations such as inspecting container information, checking logs and performance stats, renaming and updating the properties of a container, limiting resources etc. As part of this lab, we are going to launch a python based webapp for a sample voting application. Launching a container with a pre built app image To launch vote container run the following command. Don't bother about the new flag -P now. We will explain about that flag later in this chapter docker container run -idt -P schoolofdevops/vote [Output] Unable to find image 'schoolofdevops/vote:latest' locally latest: Pulling from schoolofdevops/vote Digest: sha256:9195942ea654fa8d8aeb37900be5619215c08e7e1bef0b7dfe4c04a9cc20a8c2 Status: Downloaded newer image for schoolofdevops/vote:latest 7d58ecc05754b5fd192c4ecceae334ac22565684c6923ea332bff5c88e3fca2b Lets check the status of the container docker ps -l [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7d58ecc05754 schoolofdevops/vote \"gunicorn app:app -b\u2026\" 27 seconds ago Up 26 seconds 0.0.0.0:32768->80/tcp peaceful_varahamihira Renaming the container We can rename the container by using following command docker rename 7d58ecc05754 vote [replace 7d58ecc05754 with the actual container id on your system ] We have changed container's automatically generated name to vote . This new name can be of your choice. The point to understand is this command takes two arguments. The Old_name followed by New_name Run docker ps command to check the effect of changes docker ps [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7d58ecc05754 schoolofdevops/vote \"gunicorn app:app -b\u2026\" 3 minutes ago Up 3 minutes 0.0.0.0:32768->80/tcp vote As you can see here, the container is renamed to vote . This makes referencing container in cli very much easier. Ready to vote ? Let's see what this vote application does by connecting to that application. For that we need, * Host machine's IP * Container's port which is mapped to a host's port Let's find out the port mapping of container to host. Docker provides subcommand called port which does this job docker port vote [Output] 80/tcp -> 0.0.0.0:32768 So whatever traffic the host gets in port 2368 will be mapped to container's port 32768 Let's connect to http://IP_ADDRESS:PORT to see the actual application Finding Everything about the running container This topic discusses about finding metadata of containers. These metadata include various parameters like, * State of the container * Mounts * Configuration * Network, etc., Inspecting Lets try this inspect subcommand in action docker inspect vote Data output by above command contains detailed descriptino of the container an its properties. is represented in JSON format which makes filtering these results easier. Checking the Stats Stats command This command returns a data stream of resource utilization used by containers. The flag --no-stream disables data stream and displays only first result docker stats --no-stream=true vote docker stats Top command To display the list of processes and the information about those processes that are running inside the container, we can use top command docker top vote [Output] UID PID PPID C STIME TTY TIME CMD vagrant 6219 6211 0 14:07 ? 00:00:00 npm vagrant 6275 6219 0 14:07 ? 00:00:00 sh -c node index vagrant 6276 6275 0 14:07 ? 00:00:11 node index Examine Logs Docker log command is to print the logs of the application inside the container. In our case we will see the log output of vote application docker logs vote [Output] [2018-05-01 15:36:01 +0000] [1] [INFO] Starting gunicorn 19.6.0 [2018-05-01 15:36:01 +0000] [1] [INFO] Listening at: http://0.0.0.0:80 (1) [2018-05-01 15:36:01 +0000] [1] [INFO] Using worker: sync [2018-05-01 15:36:01 +0000] [10] [INFO] Booting worker with pid: 10 [2018-05-01 15:36:01 +0000] [11] [INFO] Booting worker with pid: 11 [2018-05-01 15:36:01 +0000] [12] [INFO] Booting worker with pid: 12 [2018-05-01 15:36:01 +0000] [15] [INFO] Booting worker with pid: 15``` If you want to **follow** the log in real-time, use **-f** flag To follow the logs, docker logs -f vote Now try to read the articles available in our blog and see the log output gets updated in real-time. Hit ctrl+c to break the stream Stream events from the docker daemon Docker events serves us with the stream of events or interactions that are happening with the docker daemon. This does not stream the log data of application inside the container. That is done by docker logs command. Let us see how this command works Open an another terminal. Let us call the old terminal as Terminal 1 and the newer one as Terminal 2 . From Terminal 1, execute docker events . Now you are getting the data stream from docker daemon docker events To understand how this command actually works, let us run a container from Terminal 2 docker run -it alpine:3.4 sh If you see, in Terminal 1, the interaction with docker daemon, while running that container will be printed [Output - Terminal 1 ] 2016-09-16T13:00:20.189028004Z container create 816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923 (image=alpine:3.4, name=tiny_franklin) 2016-09-16T13:00:20.190190470Z container attach 816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923 (image=alpine:3.4, name=tiny_franklin) 2016-09-16T13:00:20.257068692Z network connect c0237b5406920749b87460597b8935adf958bae1ce997afd827921a0dbc97cdc (container=816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923, name=bridge, type=bridge) 2016-09-16T13:00:20.346533821Z container start 816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923 (image=alpine:3.4, name=tiny_franklin) 2016-09-16T13:00:20.347811877Z container resize 816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923 (height=41, image=alpine:3.4, name=tiny_franklin, width=126) Try to do various docker operations (start, stop, rm, etc.,) and see the output in Terminal 1 Attach to the container Normally, when we run a container, we use -d flag to run that container in detached mode. But sometimes you might require to make some changes inside that container. In those kind of situations, we can use attach command. This command attaches to the tty of docker container. So it will stream the output of the application. In our case, we will see the output of vote application docker attach vote Hit our blogs url several times to see the output [Output] root@swarm-03:~# docker attach vote [2018-05-01 15:44:49 +0000] [1] [INFO] Handling signal: winch You can detach from the tty by pressing ctrl-p + ctrl-q in sequence. If you haven't started your container with -it flag, then it is not possible to get your host's terminal back. In that case, If you haven't started the container with -it option, the only way you will be able to detach from the container by using ctrl-c , which kills the process, in turns the container itself. It is possible to override these keys too. For that we have to add --detach-keys flag to the command. To learn more, click on the following URL https://docs.docker.com/engine/reference/commandline/attach/ Copying files between container and client host We can copy files/directories form host to container and vice-versa Let us create a file on the host touch testfile To copy the testfile from host machine to ghsot contanier , try docker cp testfile vote:/opt This command will copy testfile to vote container's /opt directory and will not give any output. To verify the file has been copies or not, let us log into container by running, docker exec -it vote bash Change directory into /opt and list the files cd /opt ls [Output] testfile There you can see that file has been successfully copied. Now exit the container Now you may try to cp some files from the container to the host machine docker cp vote:/app . ls Controlling Resources Docker provides us the granularity to control each container's resource utilization . We have several commands in the inventory to achieve this Putting limits on Running Containers First, let us monitor the utilization docker stats --no-stream=true [Example Output] \"docker stats --no-stream=true CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS 7d58ecc05754 vote 0.02% 56.5MiB / 1.955GiB 2.82% 648B / 0B 0B / 0B 0 9fc1aec8cb6a gallant_brattain 0.00% 328KiB / 1.955GiB 0.02% 690B / 0B 0B / 0B 0 08f0242aa61c vote.2.qwxduexkwpmdnowouxjzwjwag 0.02% 56.33MiB / 1.955GiB 2.81% 1.94kB / 0B 0B / 0B 0 8469b95efc81 redis.4.s5i3kid9yohpbim05bsw59sh2 0.12% 6.223MiB / 1.955GiB 0.31% 1.36kB / 0B 0B / 0B 0 ce823d38adaf redis.1.qfx6geh6t9vuy8awq94u10m07 0.08% 6.227MiB / 1.955GiB 0.31% 7.25kB / 5.39kB 0B / 0B 0 For monitoring resources continuously, docker stats --no-stream=true You can see that Memory attribute has 0 as its value. 0 means unlimited usage of host's RAM. We can put a cap on that by using update command docker update --memory 400M --memory-swap -1 vote [Output] vote Let us check whether the change has taken effect or not docker inspect vote | grep -i memory docker stat [Output] \"Memory\": 419430400, \"KernelMemory\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": -1, As you can see, the memory utilization of the container is changed from 0 (unlimited) to 400 mb Limiting Resources while launching new containers The following resources can be limited using the update command * CPU * Memory * Disk IO * Capabilities Open two terminals, lets call them T1, and T2 In T1, start monitoring the stats docker stats [Output] CONTAINER CPU % MEM USAGE / LIMIT MM % NET I/O BLOCK I/O PIDS b28efeef41f8 0.16% 190.1 MiB / 400 MiB 47.51% 1.296 kB / 648 B 86.02 kB / 45.06 kB 0 CONTAINER CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS b28efeef41f8 0.01% 190.1 MiB / 400 MiB 47.51% 1.296 kB / 648 B 86.02 kB / 45.06 kB 0 From T2, launch two containers with different CPU shares. Default CPU shares are set to 1024. This is a relative weight. docker run -d --name st-01 schoolofdevops/stresstest stress --cpu 1 docker run -d --name st-02 -c 512 schoolofdevops/stresstest stress --cpu 1 When you launch the first container, it will use the full quota of CPU, i.e., 100% [Output - After first container launch ] CONTAINER CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS b28efeef41f8 0.01% 190.1 MiB / 400 MiB 47.51% 1.944 kB / 648 B 86.02 kB / 45.06 kB 0 764f158d6523 102.73% 2.945 MiB / 1.797 GiB 0.16% 648 B / 648 B 3.118 MB / 0 B 0 [Output - After second container lauch ] CONTAINER CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS b28efeef41f8 0.00% 190.1 MiB / 400 MiB 47.51% 2.592 kB / 648 B 86.02 kB / 45.06 kB 0 764f158d6523 66.97% 2.945 MiB / 1.797 GiB 0.16% 1.296 kB / 648 B 3.118 MB / 0 B 0 a13f98995ade 33.36% 2.945 MiB / 1.797 GiB 0.16% 648 B / 648 B 3.118 MB / 0 B 0 Observe stats in T1 Launch a couple more nodes with different cpu shares, observe how T2 stats change docker run -d --name st-03 -c 512 schoolofdevops/stresstest stress --cpu 1 docker run -d --name st-04 schoolofdevops/stresstest stress --cpu 1 [Output - After all containers are launched ] CONTAINER CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS b28efeef41f8 0.00% 190.1 MiB / 400 MiB 47.51% 3.888 kB / 648 B 86.02 kB / 45.06 kB 0 764f158d6523 32.09% 2.945 MiB / 1.797 GiB 0.16% 2.592 kB / 648 B 3.118 MB / 0 B 0 a13f98995ade 16.02% 2.945 MiB / 1.797 GiB 0.16% 1.944 kB / 648 B 3.118 MB / 0 B 0 f04e9ea5627c 16.37% 2.949 MiB / 1.797 GiB 0.16% 1.296 kB / 648 B 3.118 MB / 0 B 0 abeab389a873 31.71% 2.949 MiB / 1.797 GiB 0.16% 648 B / 648 B 3.118 MB / 0 B 0 Close the T2 terminal Exercises Try to these exercises, to get a better understanding * Put a memory limit * Set disk iops Launching Containers with Elevated Privileges When the operator executes docker run --privileged, Docker will enable to access to all devices on the host as well as set some configuration in AppArmor or SELinux to allow the container nearly all the same access to the host as processes running outside containers on the host. Example: Running a sysdig container to monitor docker Sysdig tool allows us to monitor the processes that are going on in the other containers. It is more like running a top command from one container on behalf of others. docker run -itd --name=sysdig --privileged=true \\ --volume=/var/run/docker.sock:/host/var/run/docker.sock \\ --volume=/dev:/host/dev \\ --volume=/proc:/host/proc:ro \\ --volume=/boot:/host/boot:ro \\ --volume=/lib/modules:/host/lib/modules:ro \\ --volume=/usr:/host/usr:ro \\ sysdig/sysdig:0.11.0 sysdig [Output] Unable to find image 'sysdig/sysdig:0.11.0' locally 0.11.0: Pulling from sysdig/sysdig 0f409b0f5b3d: Pull complete 64965da77fc6: Pull complete 588eeb0d4c30: Pull complete 9aa18e35b362: Pull complete cc036f2dca14: Pull complete 33400f3af946: Pull complete b39ed90e36fd: Pull complete 1fca16436380: Pull complete Digest: sha256:ee9d66a07308c5aef91f070cce5c9fb891e4fefb5da4d417e590662e34846664 Status: Downloaded newer image for sysdig/sysdig:0.11.0 6ba17cf2af7b87621b3380517af45c5785dc8cda75111f0f8c36bb83e163a120 docker exec -it sysdig bash csysdig [Output] After this, press f2 and select containers tab Now check what are the processes are running in other containers References [Resource Management in Docker by Marek Goldmann] (https://goldmann.pl/blog/2014/09/11/resource-management-in-docker/)","title":"Container Operations"},{"location":"chapter5-container_operations/#managing-containers-learning-about-common-container-operations","text":"In the previous chapter, we have learnt about container lifecycle management including how to create, launch, connect to, stop and remove containers. In this chapter, we are going to learn how to launch a container with a pre built app image and how to access the app with published ports. We will also learn about common container operations such as inspecting container information, checking logs and performance stats, renaming and updating the properties of a container, limiting resources etc. As part of this lab, we are going to launch a python based webapp for a sample voting application.","title":"Managing Containers - Learning about Common Container Operations"},{"location":"chapter5-container_operations/#launching-a-container-with-a-pre-built-app-image","text":"To launch vote container run the following command. Don't bother about the new flag -P now. We will explain about that flag later in this chapter docker container run -idt -P schoolofdevops/vote [Output] Unable to find image 'schoolofdevops/vote:latest' locally latest: Pulling from schoolofdevops/vote Digest: sha256:9195942ea654fa8d8aeb37900be5619215c08e7e1bef0b7dfe4c04a9cc20a8c2 Status: Downloaded newer image for schoolofdevops/vote:latest 7d58ecc05754b5fd192c4ecceae334ac22565684c6923ea332bff5c88e3fca2b Lets check the status of the container docker ps -l [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7d58ecc05754 schoolofdevops/vote \"gunicorn app:app -b\u2026\" 27 seconds ago Up 26 seconds 0.0.0.0:32768->80/tcp peaceful_varahamihira","title":"Launching a container with a pre built app image"},{"location":"chapter5-container_operations/#renaming-the-container","text":"We can rename the container by using following command docker rename 7d58ecc05754 vote [replace 7d58ecc05754 with the actual container id on your system ] We have changed container's automatically generated name to vote . This new name can be of your choice. The point to understand is this command takes two arguments. The Old_name followed by New_name Run docker ps command to check the effect of changes docker ps [Output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7d58ecc05754 schoolofdevops/vote \"gunicorn app:app -b\u2026\" 3 minutes ago Up 3 minutes 0.0.0.0:32768->80/tcp vote As you can see here, the container is renamed to vote . This makes referencing container in cli very much easier.","title":"Renaming the container"},{"location":"chapter5-container_operations/#ready-to-vote","text":"Let's see what this vote application does by connecting to that application. For that we need, * Host machine's IP * Container's port which is mapped to a host's port Let's find out the port mapping of container to host. Docker provides subcommand called port which does this job docker port vote [Output] 80/tcp -> 0.0.0.0:32768 So whatever traffic the host gets in port 2368 will be mapped to container's port 32768 Let's connect to http://IP_ADDRESS:PORT to see the actual application","title":"Ready to  vote ?"},{"location":"chapter5-container_operations/#finding-everything-about-the-running-container","text":"This topic discusses about finding metadata of containers. These metadata include various parameters like, * State of the container * Mounts * Configuration * Network, etc.,","title":"Finding Everything about the running  container"},{"location":"chapter5-container_operations/#inspecting","text":"Lets try this inspect subcommand in action docker inspect vote Data output by above command contains detailed descriptino of the container an its properties. is represented in JSON format which makes filtering these results easier.","title":"Inspecting"},{"location":"chapter5-container_operations/#checking-the-stats","text":"","title":"Checking the Stats"},{"location":"chapter5-container_operations/#stats-command","text":"This command returns a data stream of resource utilization used by containers. The flag --no-stream disables data stream and displays only first result docker stats --no-stream=true vote docker stats","title":"Stats command"},{"location":"chapter5-container_operations/#top-command","text":"To display the list of processes and the information about those processes that are running inside the container, we can use top command docker top vote [Output] UID PID PPID C STIME TTY TIME CMD vagrant 6219 6211 0 14:07 ? 00:00:00 npm vagrant 6275 6219 0 14:07 ? 00:00:00 sh -c node index vagrant 6276 6275 0 14:07 ? 00:00:11 node index","title":"Top command"},{"location":"chapter5-container_operations/#examine-logs","text":"Docker log command is to print the logs of the application inside the container. In our case we will see the log output of vote application docker logs vote [Output] [2018-05-01 15:36:01 +0000] [1] [INFO] Starting gunicorn 19.6.0 [2018-05-01 15:36:01 +0000] [1] [INFO] Listening at: http://0.0.0.0:80 (1) [2018-05-01 15:36:01 +0000] [1] [INFO] Using worker: sync [2018-05-01 15:36:01 +0000] [10] [INFO] Booting worker with pid: 10 [2018-05-01 15:36:01 +0000] [11] [INFO] Booting worker with pid: 11 [2018-05-01 15:36:01 +0000] [12] [INFO] Booting worker with pid: 12 [2018-05-01 15:36:01 +0000] [15] [INFO] Booting worker with pid: 15``` If you want to **follow** the log in real-time, use **-f** flag To follow the logs, docker logs -f vote Now try to read the articles available in our blog and see the log output gets updated in real-time. Hit ctrl+c to break the stream","title":"Examine Logs"},{"location":"chapter5-container_operations/#stream-events-from-the-docker-daemon","text":"Docker events serves us with the stream of events or interactions that are happening with the docker daemon. This does not stream the log data of application inside the container. That is done by docker logs command. Let us see how this command works Open an another terminal. Let us call the old terminal as Terminal 1 and the newer one as Terminal 2 . From Terminal 1, execute docker events . Now you are getting the data stream from docker daemon docker events To understand how this command actually works, let us run a container from Terminal 2 docker run -it alpine:3.4 sh If you see, in Terminal 1, the interaction with docker daemon, while running that container will be printed [Output - Terminal 1 ] 2016-09-16T13:00:20.189028004Z container create 816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923 (image=alpine:3.4, name=tiny_franklin) 2016-09-16T13:00:20.190190470Z container attach 816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923 (image=alpine:3.4, name=tiny_franklin) 2016-09-16T13:00:20.257068692Z network connect c0237b5406920749b87460597b8935adf958bae1ce997afd827921a0dbc97cdc (container=816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923, name=bridge, type=bridge) 2016-09-16T13:00:20.346533821Z container start 816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923 (image=alpine:3.4, name=tiny_franklin) 2016-09-16T13:00:20.347811877Z container resize 816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923 (height=41, image=alpine:3.4, name=tiny_franklin, width=126) Try to do various docker operations (start, stop, rm, etc.,) and see the output in Terminal 1","title":"Stream events from the docker daemon"},{"location":"chapter5-container_operations/#attach-to-the-container","text":"Normally, when we run a container, we use -d flag to run that container in detached mode. But sometimes you might require to make some changes inside that container. In those kind of situations, we can use attach command. This command attaches to the tty of docker container. So it will stream the output of the application. In our case, we will see the output of vote application docker attach vote Hit our blogs url several times to see the output [Output] root@swarm-03:~# docker attach vote [2018-05-01 15:44:49 +0000] [1] [INFO] Handling signal: winch You can detach from the tty by pressing ctrl-p + ctrl-q in sequence. If you haven't started your container with -it flag, then it is not possible to get your host's terminal back. In that case, If you haven't started the container with -it option, the only way you will be able to detach from the container by using ctrl-c , which kills the process, in turns the container itself. It is possible to override these keys too. For that we have to add --detach-keys flag to the command. To learn more, click on the following URL https://docs.docker.com/engine/reference/commandline/attach/","title":"Attach to the container"},{"location":"chapter5-container_operations/#copying-files-between-container-and-client-host","text":"We can copy files/directories form host to container and vice-versa Let us create a file on the host touch testfile To copy the testfile from host machine to ghsot contanier , try docker cp testfile vote:/opt This command will copy testfile to vote container's /opt directory and will not give any output. To verify the file has been copies or not, let us log into container by running, docker exec -it vote bash Change directory into /opt and list the files cd /opt ls [Output] testfile There you can see that file has been successfully copied. Now exit the container Now you may try to cp some files from the container to the host machine docker cp vote:/app . ls","title":"Copying files between container and client host"},{"location":"chapter5-container_operations/#controlling-resources","text":"Docker provides us the granularity to control each container's resource utilization . We have several commands in the inventory to achieve this","title":"Controlling Resources"},{"location":"chapter5-container_operations/#putting-limits-on-running-containers","text":"First, let us monitor the utilization docker stats --no-stream=true [Example Output] \"docker stats --no-stream=true CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS 7d58ecc05754 vote 0.02% 56.5MiB / 1.955GiB 2.82% 648B / 0B 0B / 0B 0 9fc1aec8cb6a gallant_brattain 0.00% 328KiB / 1.955GiB 0.02% 690B / 0B 0B / 0B 0 08f0242aa61c vote.2.qwxduexkwpmdnowouxjzwjwag 0.02% 56.33MiB / 1.955GiB 2.81% 1.94kB / 0B 0B / 0B 0 8469b95efc81 redis.4.s5i3kid9yohpbim05bsw59sh2 0.12% 6.223MiB / 1.955GiB 0.31% 1.36kB / 0B 0B / 0B 0 ce823d38adaf redis.1.qfx6geh6t9vuy8awq94u10m07 0.08% 6.227MiB / 1.955GiB 0.31% 7.25kB / 5.39kB 0B / 0B 0 For monitoring resources continuously, docker stats --no-stream=true You can see that Memory attribute has 0 as its value. 0 means unlimited usage of host's RAM. We can put a cap on that by using update command docker update --memory 400M --memory-swap -1 vote [Output] vote Let us check whether the change has taken effect or not docker inspect vote | grep -i memory docker stat [Output] \"Memory\": 419430400, \"KernelMemory\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": -1, As you can see, the memory utilization of the container is changed from 0 (unlimited) to 400 mb","title":"Putting limits on Running Containers"},{"location":"chapter5-container_operations/#limiting-resources-while-launching-new-containers","text":"The following resources can be limited using the update command * CPU * Memory * Disk IO * Capabilities Open two terminals, lets call them T1, and T2 In T1, start monitoring the stats docker stats [Output] CONTAINER CPU % MEM USAGE / LIMIT MM % NET I/O BLOCK I/O PIDS b28efeef41f8 0.16% 190.1 MiB / 400 MiB 47.51% 1.296 kB / 648 B 86.02 kB / 45.06 kB 0 CONTAINER CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS b28efeef41f8 0.01% 190.1 MiB / 400 MiB 47.51% 1.296 kB / 648 B 86.02 kB / 45.06 kB 0 From T2, launch two containers with different CPU shares. Default CPU shares are set to 1024. This is a relative weight. docker run -d --name st-01 schoolofdevops/stresstest stress --cpu 1 docker run -d --name st-02 -c 512 schoolofdevops/stresstest stress --cpu 1 When you launch the first container, it will use the full quota of CPU, i.e., 100% [Output - After first container launch ] CONTAINER CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS b28efeef41f8 0.01% 190.1 MiB / 400 MiB 47.51% 1.944 kB / 648 B 86.02 kB / 45.06 kB 0 764f158d6523 102.73% 2.945 MiB / 1.797 GiB 0.16% 648 B / 648 B 3.118 MB / 0 B 0 [Output - After second container lauch ] CONTAINER CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS b28efeef41f8 0.00% 190.1 MiB / 400 MiB 47.51% 2.592 kB / 648 B 86.02 kB / 45.06 kB 0 764f158d6523 66.97% 2.945 MiB / 1.797 GiB 0.16% 1.296 kB / 648 B 3.118 MB / 0 B 0 a13f98995ade 33.36% 2.945 MiB / 1.797 GiB 0.16% 648 B / 648 B 3.118 MB / 0 B 0 Observe stats in T1 Launch a couple more nodes with different cpu shares, observe how T2 stats change docker run -d --name st-03 -c 512 schoolofdevops/stresstest stress --cpu 1 docker run -d --name st-04 schoolofdevops/stresstest stress --cpu 1 [Output - After all containers are launched ] CONTAINER CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS b28efeef41f8 0.00% 190.1 MiB / 400 MiB 47.51% 3.888 kB / 648 B 86.02 kB / 45.06 kB 0 764f158d6523 32.09% 2.945 MiB / 1.797 GiB 0.16% 2.592 kB / 648 B 3.118 MB / 0 B 0 a13f98995ade 16.02% 2.945 MiB / 1.797 GiB 0.16% 1.944 kB / 648 B 3.118 MB / 0 B 0 f04e9ea5627c 16.37% 2.949 MiB / 1.797 GiB 0.16% 1.296 kB / 648 B 3.118 MB / 0 B 0 abeab389a873 31.71% 2.949 MiB / 1.797 GiB 0.16% 648 B / 648 B 3.118 MB / 0 B 0 Close the T2 terminal","title":"Limiting Resources while launching new containers"},{"location":"chapter5-container_operations/#exercises","text":"Try to these exercises, to get a better understanding * Put a memory limit * Set disk iops","title":"Exercises"},{"location":"chapter5-container_operations/#launching-containers-with-elevated-privileges","text":"When the operator executes docker run --privileged, Docker will enable to access to all devices on the host as well as set some configuration in AppArmor or SELinux to allow the container nearly all the same access to the host as processes running outside containers on the host.","title":"Launching Containers with Elevated  Privileges"},{"location":"chapter5-container_operations/#example","text":"","title":"Example:"},{"location":"chapter5-container_operations/#running-a-sysdig-container-to-monitor-docker","text":"Sysdig tool allows us to monitor the processes that are going on in the other containers. It is more like running a top command from one container on behalf of others. docker run -itd --name=sysdig --privileged=true \\ --volume=/var/run/docker.sock:/host/var/run/docker.sock \\ --volume=/dev:/host/dev \\ --volume=/proc:/host/proc:ro \\ --volume=/boot:/host/boot:ro \\ --volume=/lib/modules:/host/lib/modules:ro \\ --volume=/usr:/host/usr:ro \\ sysdig/sysdig:0.11.0 sysdig [Output] Unable to find image 'sysdig/sysdig:0.11.0' locally 0.11.0: Pulling from sysdig/sysdig 0f409b0f5b3d: Pull complete 64965da77fc6: Pull complete 588eeb0d4c30: Pull complete 9aa18e35b362: Pull complete cc036f2dca14: Pull complete 33400f3af946: Pull complete b39ed90e36fd: Pull complete 1fca16436380: Pull complete Digest: sha256:ee9d66a07308c5aef91f070cce5c9fb891e4fefb5da4d417e590662e34846664 Status: Downloaded newer image for sysdig/sysdig:0.11.0 6ba17cf2af7b87621b3380517af45c5785dc8cda75111f0f8c36bb83e163a120 docker exec -it sysdig bash csysdig [Output] After this, press f2 and select containers tab Now check what are the processes are running in other containers","title":"Running a sysdig container to monitor docker"},{"location":"chapter5-container_operations/#references","text":"[Resource Management in Docker by Marek Goldmann] (https://goldmann.pl/blog/2014/09/11/resource-management-in-docker/)","title":"References"},{"location":"chapter6-dockerizing-your-apps/","text":"Dockerizing your Applications : Building Images and Working with Registries In the previous session, we have learnt about various container operations such as running containers from pre built images, port mapping, inspecting and updating containers, limiting resources etc., In this chapter, we are going to learn about how to build containers for your individual applications, as well as how to work with docker hub registry to host and distribute the images. Lab: Registering with the DockerHub Since we are going to start working with the registry, build and push images to it later, its essential to have our own account on the registry. For the purpose of this tutorial, we are going to use the hosted registry i.e. Dockerhub. Steps to create Dockerhub account Step 1: Visit the following link and sign up with your email id https://hub.docker.com/ Step 2: Check your email inbox and check the activation email sent by docker team Step 3: After clicking on the activation link, you will be redirected to a log in page. Enter your credentials and log in You will be launched to Dockerhub main page. Now the registration process is complete and you have account in Dockerhub! Lab: Building Docker Images - A manual approach Before we start building automated images, we are going to create a docker image by hand. We have already used the pre built image from the registry in the last session. In this session, however, we are going to create our own image with ghost installed. Since Ghost is a node.js based application, we will base our work on existing official image for node Clone Repository for Java worker app git clone https://github.com/schoolofdevops/example-voting-app.git Launch a intermediate container to install worker app Create a Container with schoolofdevops/voteapp-mvn:v1 image docker run -idt --name interim schoolofdevops/voteapp-mvn sh Copy over the Source Code cd example-voting-app/worker docker container cp . interim:/code Connect to container to compile and package the code docker exec -it interim sh mvn package Verify jarfile has been built ls target/ java -jar target/worker-jar-with-dependencies.jar [sample output] /code # java -jar target/worker-jar-with-dependencies.jar Waiting for redis Waiting for redis Waiting for redis Waiting for redis Waiting for redis Waiting for redis ^c [use ^c to exit] The above is the expected output. The worker app keeps waiting for redis and then later db in a loop. Move the artifact, remove source code mv target/worker-jar-with-dependencies.jar /run/worker.jar rm -rf /code/* exit Commit container to an image Exit from the container shell Note container ID Commit the container into a image as, docker container commit interim <docker hub user id >/worker:v1 Test before pushing by launching container with the packaged app docker run --rm -it <docker hub user id >/worker:v1 java -jar /run/worker.jar Push Image to registry Before you push the image, you need to be logged in to the registry, with the docker hub id created earlier. Login using the following command, docker login To push the image, first list it, docker image ls [Sample Output] REPOSITORY TAG IMAGE ID CREATED SIZE initcron/worker v2 90cbeb6539df 18 minutes ago 194MB initcron/worker v1 c0199f782489 34 minutes ago 189MB To push the image, docker push <dockrhub user id>/worker:v1 Lab: Building Images with Dockerfile Now, lets build the same image, this time with Dockerfile. To do this, create a file by name Dockerfile in the root of the source code. file: example-voting-app/worker/Dockerfile FROM schoolofdevops/maven WORKDIR /app COPY . . RUN mvn package && \\ mv target/worker-jar-with-dependencies.jar /run/worker.jar && \\ rm -rf /app/* CMD java -jar /run/worker.jar Lets now build the image cd example-voting-app/worker docker image build -t <dockrhub user id>/worker:v2 . docker image ls Try building again, docker image build -t <dockrhub user id>/worker:v2 . This time, it does not build everything, but uses cache. Testing the image docker container run --rm -it <dockrhub user id>/worker:v2 Tag the image as latest, docker image tag <dockrhub user id>/worker:v2 <dockrhub user id>/worker:latest docker image ls Finally, publish it to the registry, docker image push <dockrhub user id>/worker:latest docker image push <dockrhub user id>/worker References Building Base Images: https://docs.docker.com/develop/develop-images/baseimages/","title":"Dockerizing your Applications : Building Images and Working with Registries"},{"location":"chapter6-dockerizing-your-apps/#dockerizing-your-applications-building-images-and-working-with-registries","text":"In the previous session, we have learnt about various container operations such as running containers from pre built images, port mapping, inspecting and updating containers, limiting resources etc., In this chapter, we are going to learn about how to build containers for your individual applications, as well as how to work with docker hub registry to host and distribute the images.","title":"Dockerizing your Applications : Building Images and Working with Registries"},{"location":"chapter6-dockerizing-your-apps/#lab-registering-with-the-dockerhub","text":"Since we are going to start working with the registry, build and push images to it later, its essential to have our own account on the registry. For the purpose of this tutorial, we are going to use the hosted registry i.e. Dockerhub. Steps to create Dockerhub account","title":"Lab: Registering with the DockerHub"},{"location":"chapter6-dockerizing-your-apps/#step-1","text":"Visit the following link and sign up with your email id https://hub.docker.com/","title":"Step 1:"},{"location":"chapter6-dockerizing-your-apps/#step-2","text":"Check your email inbox and check the activation email sent by docker team","title":"Step 2:"},{"location":"chapter6-dockerizing-your-apps/#step-3","text":"After clicking on the activation link, you will be redirected to a log in page. Enter your credentials and log in You will be launched to Dockerhub main page. Now the registration process is complete and you have account in Dockerhub!","title":"Step 3:"},{"location":"chapter6-dockerizing-your-apps/#lab-building-docker-images-a-manual-approach","text":"Before we start building automated images, we are going to create a docker image by hand. We have already used the pre built image from the registry in the last session. In this session, however, we are going to create our own image with ghost installed. Since Ghost is a node.js based application, we will base our work on existing official image for node","title":"Lab: Building Docker Images - A manual approach"},{"location":"chapter6-dockerizing-your-apps/#clone-repository-for-java-worker-app","text":"git clone https://github.com/schoolofdevops/example-voting-app.git","title":"Clone Repository for Java worker app"},{"location":"chapter6-dockerizing-your-apps/#launch-a-intermediate-container-to-install-worker-app","text":"Create a Container with schoolofdevops/voteapp-mvn:v1 image docker run -idt --name interim schoolofdevops/voteapp-mvn sh","title":"Launch a intermediate container to install worker app"},{"location":"chapter6-dockerizing-your-apps/#copy-over-the-source-code","text":"cd example-voting-app/worker docker container cp . interim:/code Connect to container to compile and package the code docker exec -it interim sh mvn package","title":"Copy over the Source Code"},{"location":"chapter6-dockerizing-your-apps/#verify-jarfile-has-been-built","text":"ls target/ java -jar target/worker-jar-with-dependencies.jar [sample output] /code # java -jar target/worker-jar-with-dependencies.jar Waiting for redis Waiting for redis Waiting for redis Waiting for redis Waiting for redis Waiting for redis ^c [use ^c to exit] The above is the expected output. The worker app keeps waiting for redis and then later db in a loop. Move the artifact, remove source code mv target/worker-jar-with-dependencies.jar /run/worker.jar rm -rf /code/* exit Commit container to an image Exit from the container shell Note container ID Commit the container into a image as, docker container commit interim <docker hub user id >/worker:v1 Test before pushing by launching container with the packaged app docker run --rm -it <docker hub user id >/worker:v1 java -jar /run/worker.jar","title":"Verify jarfile has been built"},{"location":"chapter6-dockerizing-your-apps/#push-image-to-registry","text":"Before you push the image, you need to be logged in to the registry, with the docker hub id created earlier. Login using the following command, docker login To push the image, first list it, docker image ls [Sample Output] REPOSITORY TAG IMAGE ID CREATED SIZE initcron/worker v2 90cbeb6539df 18 minutes ago 194MB initcron/worker v1 c0199f782489 34 minutes ago 189MB To push the image, docker push <dockrhub user id>/worker:v1","title":"Push Image to registry"},{"location":"chapter6-dockerizing-your-apps/#lab-building-images-with-dockerfile","text":"Now, lets build the same image, this time with Dockerfile. To do this, create a file by name Dockerfile in the root of the source code. file: example-voting-app/worker/Dockerfile FROM schoolofdevops/maven WORKDIR /app COPY . . RUN mvn package && \\ mv target/worker-jar-with-dependencies.jar /run/worker.jar && \\ rm -rf /app/* CMD java -jar /run/worker.jar Lets now build the image cd example-voting-app/worker docker image build -t <dockrhub user id>/worker:v2 . docker image ls Try building again, docker image build -t <dockrhub user id>/worker:v2 . This time, it does not build everything, but uses cache. Testing the image docker container run --rm -it <dockrhub user id>/worker:v2 Tag the image as latest, docker image tag <dockrhub user id>/worker:v2 <dockrhub user id>/worker:latest docker image ls Finally, publish it to the registry, docker image push <dockrhub user id>/worker:latest docker image push <dockrhub user id>/worker","title":"Lab: Building Images with Dockerfile"},{"location":"chapter6-dockerizing-your-apps/#references","text":"Building Base Images: https://docs.docker.com/develop/develop-images/baseimages/","title":"References"},{"location":"chapter7-linking_containers/","text":"Building Application Stacks - Defining and Running Multi Container Apps Lab: Creating a Docker Compose Stack for the Vote Application Lets first launch redis and vote independently and see if they automatically connect. docker container run -idt --name redis redis:alpine docker container run -idt --name vote -P schoolofdevops/vote Try registering a vote with the voteapp UI. Does it work? You could also try if vote is able to discover redis by running docker exec vote ping redis Linking services Remove vote container created above if any, and re launch it with the link. docker container rm -f vote docker container run -idt --name vote --link redis:redis -P schoolofdevops/vote Launch worker app as well with the link docker container run -idt --name worker --link redis:redis -P schoolofdevops/vote-worker docker logs worker Launching inter linked services with Compose spec Lets now create a docker-compose spec and launch the services with docker-compose utility. Create a directory to keep the compose files. Lets say stack mkdir stack cd stack file: docker-compose.yml vote: image: schoolofdevops/vote links: - redis:redis ports: - 80 redis: image: redis:alpine worker: image: schoolofdevops/vote-worker links: - redis:redis Syntax check docker-compose config Now launch it with docker-compose up -d docker-compose ps file: docker-compose-v3.yml version: \"3\" networks: vote: driver: bridge services: vote: image: schoolofdevops/vote ports: - 80 networks: - vote depends_on: - redis redis: image: redis:alpine networks: - vote worker: image: schoolofdevops/vote-worker networks: - vote depends_on: - redis Launch the new stack with, docker-compose -f docker-compose-v3.yml up -d docker-compose -f docker-compose-v3.yml ps docker-compose -f docker-compose-v3.yml down","title":"Connecting apps with Docker Compose"},{"location":"chapter7-linking_containers/#building-application-stacks-defining-and-running-multi-container-apps","text":"","title":"Building Application Stacks - Defining and Running Multi Container Apps"},{"location":"chapter7-linking_containers/#lab-creating-a-docker-compose-stack-for-the-vote-application","text":"Lets first launch redis and vote independently and see if they automatically connect. docker container run -idt --name redis redis:alpine docker container run -idt --name vote -P schoolofdevops/vote Try registering a vote with the voteapp UI. Does it work? You could also try if vote is able to discover redis by running docker exec vote ping redis","title":"Lab: Creating a Docker Compose Stack for the Vote Application"},{"location":"chapter7-linking_containers/#linking-services","text":"Remove vote container created above if any, and re launch it with the link. docker container rm -f vote docker container run -idt --name vote --link redis:redis -P schoolofdevops/vote Launch worker app as well with the link docker container run -idt --name worker --link redis:redis -P schoolofdevops/vote-worker docker logs worker","title":"Linking services"},{"location":"chapter7-linking_containers/#launching-inter-linked-services-with-compose-spec","text":"Lets now create a docker-compose spec and launch the services with docker-compose utility. Create a directory to keep the compose files. Lets say stack mkdir stack cd stack file: docker-compose.yml vote: image: schoolofdevops/vote links: - redis:redis ports: - 80 redis: image: redis:alpine worker: image: schoolofdevops/vote-worker links: - redis:redis Syntax check docker-compose config Now launch it with docker-compose up -d docker-compose ps file: docker-compose-v3.yml version: \"3\" networks: vote: driver: bridge services: vote: image: schoolofdevops/vote ports: - 80 networks: - vote depends_on: - redis redis: image: redis:alpine networks: - vote worker: image: schoolofdevops/vote-worker networks: - vote depends_on: - redis Launch the new stack with, docker-compose -f docker-compose-v3.yml up -d docker-compose -f docker-compose-v3.yml ps docker-compose -f docker-compose-v3.yml down","title":"Launching inter linked services with Compose spec"},{"location":"chapterx-docker-networking/","text":"Lab: Docker Networking Host Networking bridge host peer none Examine the existing network docker network ls NETWORK ID NAME DRIVER SCOPE b3d405dd37e4 bridge bridge local 7527c821537c host host local 773bea4ca095 none null local Creating new network docker network create -d bridge mynet validate docker network ls NETWORK ID NAME DRIVER SCOPE b3d405dd37e4 bridge bridge local 7527c821537c host host local 4e0d9b1a39f8 mynet bridge local 773bea4ca095 none null local docker network inspect mynet [ { \"Name\": \"mynet\", \"Id\": \"4e0d9b1a39f859af4811986534c91527146bc9d2ce178e5de02473c0f8ce62d5\", \"Created\": \"2018-05-03T04:44:19.187296148Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": {}, \"Config\": [ { \"Subnet\": \"172.18.0.0/16\", \"Gateway\": \"172.18.0.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": { \"Network\": \"\" }, \"ConfigOnly\": false, \"Containers\": {}, \"Options\": {}, \"Labels\": {} } ] Launching containers in different bridges Launch two containers nt01 and nt02 in default bridge network docker container run -idt --name nt01 alpine sh docker container run -idt --name nt02 alpine sh Launch two containers nt03 and nt04 in mynet bridge network docker container run -idt --name nt03 --net mynet alpine sh docker container run -idt --name nt04 --net mynet alpine sh Now, lets examine if they can interconnect, docker exec nt01 ifconfig eth0 docker exec nt02 ifconfig eth0 docker exec nt03 ifconfig eth0 docker exec nt04 ifconfig eth0 This is what I see nt01 : 172.17.0.18 nt02 : 172.17.0.19 nt03 : 172.18.0.2 nt04 : 172.18.0.3 Create a table with the ips on your host. Once you do that, Try to, ping from nt01 to nt02 ping from nt01 to nt03 ping from nt03 to nt04 ping from nt03 to nt02 e.g. [replace ip addresses as per your setup] docker exec nt01 ping 172.17.0.19 docker exec nt01 ping 172.18.0.2 docker exec nt03 ping 172.17.0.19 docker exec nt03 ping 172.18.0.2 Clearly, these two are two differnt subnets/networks even though running on the same host. nt01 and nt02 can connect with each other, whereas nt03 and nt04 can connect. But connection between containers attached to two different subnets is not possible. Using None Network Driver docker container run -idt --name nt05 --net none alpine sh docker exec -it nt05 sh ifconfig Using Host Network Driver docker container run -idt --name nt05 --net host alpine sh docker exec -it nt05 sh ifconfig Observe docker bridge, routing and port mapping Exercise: Read about netshoot utility here Launch netshoot and connect to the host network docker run -it --net host --privileged nicolaka/netshoot Examine port mapping, iptables -nvL -t nat Traverse host port to container ip and port. Observe docker bridge and routing with the following command, brctl show ip route show","title":"Docker Single Host Networking"},{"location":"chapterx-docker-networking/#lab-docker-networking","text":"","title":"Lab: Docker Networking"},{"location":"chapterx-docker-networking/#host-networking","text":"bridge host peer none Examine the existing network docker network ls NETWORK ID NAME DRIVER SCOPE b3d405dd37e4 bridge bridge local 7527c821537c host host local 773bea4ca095 none null local Creating new network docker network create -d bridge mynet validate docker network ls NETWORK ID NAME DRIVER SCOPE b3d405dd37e4 bridge bridge local 7527c821537c host host local 4e0d9b1a39f8 mynet bridge local 773bea4ca095 none null local docker network inspect mynet [ { \"Name\": \"mynet\", \"Id\": \"4e0d9b1a39f859af4811986534c91527146bc9d2ce178e5de02473c0f8ce62d5\", \"Created\": \"2018-05-03T04:44:19.187296148Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": {}, \"Config\": [ { \"Subnet\": \"172.18.0.0/16\", \"Gateway\": \"172.18.0.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": { \"Network\": \"\" }, \"ConfigOnly\": false, \"Containers\": {}, \"Options\": {}, \"Labels\": {} } ]","title":"Host Networking"},{"location":"chapterx-docker-networking/#launching-containers-in-different-bridges","text":"Launch two containers nt01 and nt02 in default bridge network docker container run -idt --name nt01 alpine sh docker container run -idt --name nt02 alpine sh Launch two containers nt03 and nt04 in mynet bridge network docker container run -idt --name nt03 --net mynet alpine sh docker container run -idt --name nt04 --net mynet alpine sh Now, lets examine if they can interconnect, docker exec nt01 ifconfig eth0 docker exec nt02 ifconfig eth0 docker exec nt03 ifconfig eth0 docker exec nt04 ifconfig eth0 This is what I see nt01 : 172.17.0.18 nt02 : 172.17.0.19 nt03 : 172.18.0.2 nt04 : 172.18.0.3 Create a table with the ips on your host. Once you do that, Try to, ping from nt01 to nt02 ping from nt01 to nt03 ping from nt03 to nt04 ping from nt03 to nt02 e.g. [replace ip addresses as per your setup] docker exec nt01 ping 172.17.0.19 docker exec nt01 ping 172.18.0.2 docker exec nt03 ping 172.17.0.19 docker exec nt03 ping 172.18.0.2 Clearly, these two are two differnt subnets/networks even though running on the same host. nt01 and nt02 can connect with each other, whereas nt03 and nt04 can connect. But connection between containers attached to two different subnets is not possible.","title":"Launching containers in different bridges"},{"location":"chapterx-docker-networking/#using-none-network-driver","text":"docker container run -idt --name nt05 --net none alpine sh docker exec -it nt05 sh ifconfig","title":"Using None Network Driver"},{"location":"chapterx-docker-networking/#using-host-network-driver","text":"docker container run -idt --name nt05 --net host alpine sh docker exec -it nt05 sh ifconfig","title":"Using Host Network Driver"},{"location":"chapterx-docker-networking/#observe-docker-bridge-routing-and-port-mapping","text":"Exercise: Read about netshoot utility here Launch netshoot and connect to the host network docker run -it --net host --privileged nicolaka/netshoot Examine port mapping, iptables -nvL -t nat Traverse host port to container ip and port. Observe docker bridge and routing with the following command, brctl show ip route show","title":"Observe docker bridge, routing and port mapping"},{"location":"distributing_images/","text":"Distributing Docker Images and Advanced Image Operations Using public registry (Docker Hub) Public Repos Private Repos Automated Builds Using Private Registry VMWare Harbor Offline Distribution docker image save docker image load Flatten Images docker container export docker image import By now, you would have already learnt how to use Docker Hub as a public registry to distribute images, with public repositories. In addition to the public repos, you could also create private repo on Docker Hub and share access with specific group of people. Offline distribution of images docker image pull schoolofdevops/vote docker image save schoolofdevops/vote -o schoolofdevops_vote.tar docker image rm schoolofdevops/vote docker image ls docker image load -i schoolofdevops_vote.tar Flattening Images docker image history schoolofdevops/vote docker container run -idt -P --name vote schoolofdevops/vote docker container export vote -o schoolofdevops_vote_exported.tar docker image import schoolofdevops_vote_exported.tar schoolofdevops/vote:imported docker image history schoolofdevops/vote:imported","title":"Distributing Images"},{"location":"distributing_images/#distributing-docker-images-and-advanced-image-operations","text":"Using public registry (Docker Hub) Public Repos Private Repos Automated Builds Using Private Registry VMWare Harbor Offline Distribution docker image save docker image load Flatten Images docker container export docker image import By now, you would have already learnt how to use Docker Hub as a public registry to distribute images, with public repositories. In addition to the public repos, you could also create private repo on Docker Hub and share access with specific group of people.","title":"Distributing Docker Images and Advanced Image Operations"},{"location":"distributing_images/#offline-distribution-of-images","text":"docker image pull schoolofdevops/vote docker image save schoolofdevops/vote -o schoolofdevops_vote.tar docker image rm schoolofdevops/vote docker image ls docker image load -i schoolofdevops_vote.tar","title":"Offline distribution of images"},{"location":"distributing_images/#flattening-images","text":"docker image history schoolofdevops/vote docker container run -idt -P --name vote schoolofdevops/vote docker container export vote -o schoolofdevops_vote_exported.tar docker image import schoolofdevops_vote_exported.tar schoolofdevops/vote:imported docker image history schoolofdevops/vote:imported","title":"Flattening Images"},{"location":"docker-security/","text":"References: Docker Security Labs : https://github.com/docker/labs/tree/master/security Docker Bench Security Scan : https://github.com/docker/docker-bench-security Content Trust : https://github.com/docker/labs/blob/master/security/trust/README.md User Namespaces : https://github.com/docker/labs/blob/master/security/userns/README.md Seccomp: https://docs.docker.com/engine/security/seccomp/#significant-syscalls-blocked-by-the-default-profile Secrets: https://github.com/docker/labs/blob/master/security/secrets/README.md","title":"Docker Security"},{"location":"docker-security/#references","text":"Docker Security Labs : https://github.com/docker/labs/tree/master/security Docker Bench Security Scan : https://github.com/docker/docker-bench-security Content Trust : https://github.com/docker/labs/blob/master/security/trust/README.md User Namespaces : https://github.com/docker/labs/blob/master/security/userns/README.md Seccomp: https://docs.docker.com/engine/security/seccomp/#significant-syscalls-blocked-by-the-default-profile Secrets: https://github.com/docker/labs/blob/master/security/secrets/README.md","title":"References:"},{"location":"docker-swarm-ucp-troubleshooting/","text":"Stack/Layer Based Troubleshooting UCP Troubleshooting [UCP Architecture] (https://docs.docker.com/datacenter/ucp/2.2/guides/architecture/) [UCP Node States[( https://docs.docker.com/ee/ucp/admin/monitor-and-troubleshoot/troubleshoot-node-messages/#ucp-node-states) Troubleshooting UCP Cluster Matrix Swarm Configurations Troubleshooting On UCP Node, check the key value cluster health docker exec -it ucp-kv etcdctl \\ --endpoint https://127.0.0.1:2379 \\ --ca-file /etc/docker/ssl/ca.pem \\ --cert-file /etc/docker/ssl/cert.pem \\ --key-file /etc/docker/ssl/key.pem \\ cluster-health Rethink DB Status NODE_ADDRESS=$(docker info --format '{{.Swarm.NodeAddr}}') VERSION=$(docker image ls --format '{{.Tag}}' docker/ucp-auth | head -n 1) docker container run --rm -v ucp-auth-store-certs:/tls docker/ucp-auth:${VERSION} --db-addr=${NODE_ADDRESS}:12383 db-status DTR Troubleshooting Health Checks https://dtr.schoolofdevops.org/_ping https://dtr.schoolofdevops.org/nginx_status https://dtr.schoolofdevops.org/api/v0/meta/cluster_status DTR Overlay and RethinkDB Troubleshooting https://docs.docker.com/ee/dtr/admin/monitor-and-troubleshoot/troubleshoot-with-logs/ Swarm Troubleshooting Node Level docker node ls docker node inspect <node> docker node update --availability drain <node> docker node rm <node> Service Level docker service ls docker service ps <service> docker service inspect <service> docker service logs <service> Look for CreatedAt , UpdatedAt to corroborate with start of the issue. Tasks/Containers docker inspect <container> docker logs <container> Finding Docker Daemon Logs Ubuntu (old using upstart ) - /var/log/upstart/docker.log Ubuntu (new using systemd ) - sudo journalctl -fu docker.service Boot2Docker - /var/log/docker.log Debian GNU/Linux - /var/log/daemon.log CentOS - /var/log/daemon.log | grep docker CoreOS - journalctl -u docker.service Fedora - journalctl -u docker.service Red Hat Enterprise Linux Server - /var/log/messages | grep docker OpenSuSE - journalctl -u docker.service OSX - ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/log/d\u200c\u200bocker.log Windows - Get-EventLog -LogName Application -Source Docker -After (Get-Date).AddMinutes(-5) | Sort-Object Time System Troubleshooting File Descriptors cat /proc/sys/fs/file-max cat /proc/sys/fs/file-nr [output] 1632 0 202648 where, 1632: currently allocated file descriptors 0: free allocated file descriptors 202648 : max file descriptors To update the limit ulimit -n 99999 sysctl -w fs.file-max=100000 docker run --ulimit nofile=90000:90000 <image-tag> To check open files lsof lsof | wc -l lsof | grep <pid> References Etcd and RethinkDB Troubleshooting: https://docs.docker.com/datacenter/ucp/2.2/guides/admin/monitor-and-troubleshoot/troubleshoot-configurations/#check-the-status-of-the-database Where are docker daemon logs, stackoverflow discussion https://stackoverflow.com/questions/30969435/where-is-the-docker-daemon-log?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa File Descriptors: https://www.netadmintools.com/art295.html","title":"SWARM, UCP, DTR Troubleshooting"},{"location":"docker-swarm-ucp-troubleshooting/#stacklayer-based-troubleshooting","text":"","title":"Stack/Layer  Based Troubleshooting"},{"location":"docker-swarm-ucp-troubleshooting/#ucp-troubleshooting","text":"[UCP Architecture] (https://docs.docker.com/datacenter/ucp/2.2/guides/architecture/) [UCP Node States[( https://docs.docker.com/ee/ucp/admin/monitor-and-troubleshoot/troubleshoot-node-messages/#ucp-node-states) Troubleshooting UCP Cluster Matrix","title":"UCP Troubleshooting"},{"location":"docker-swarm-ucp-troubleshooting/#swarm-configurations-troubleshooting","text":"On UCP Node, check the key value cluster health docker exec -it ucp-kv etcdctl \\ --endpoint https://127.0.0.1:2379 \\ --ca-file /etc/docker/ssl/ca.pem \\ --cert-file /etc/docker/ssl/cert.pem \\ --key-file /etc/docker/ssl/key.pem \\ cluster-health","title":"Swarm Configurations Troubleshooting"},{"location":"docker-swarm-ucp-troubleshooting/#rethink-db-status","text":"NODE_ADDRESS=$(docker info --format '{{.Swarm.NodeAddr}}') VERSION=$(docker image ls --format '{{.Tag}}' docker/ucp-auth | head -n 1) docker container run --rm -v ucp-auth-store-certs:/tls docker/ucp-auth:${VERSION} --db-addr=${NODE_ADDRESS}:12383 db-status","title":"Rethink DB Status"},{"location":"docker-swarm-ucp-troubleshooting/#dtr-troubleshooting","text":"Health Checks https://dtr.schoolofdevops.org/_ping https://dtr.schoolofdevops.org/nginx_status https://dtr.schoolofdevops.org/api/v0/meta/cluster_status DTR Overlay and RethinkDB Troubleshooting https://docs.docker.com/ee/dtr/admin/monitor-and-troubleshoot/troubleshoot-with-logs/","title":"DTR Troubleshooting"},{"location":"docker-swarm-ucp-troubleshooting/#swarm-troubleshooting","text":"Node Level docker node ls docker node inspect <node> docker node update --availability drain <node> docker node rm <node> Service Level docker service ls docker service ps <service> docker service inspect <service> docker service logs <service> Look for CreatedAt , UpdatedAt to corroborate with start of the issue. Tasks/Containers docker inspect <container> docker logs <container>","title":"Swarm Troubleshooting"},{"location":"docker-swarm-ucp-troubleshooting/#finding-docker-daemon-logs","text":"Ubuntu (old using upstart ) - /var/log/upstart/docker.log Ubuntu (new using systemd ) - sudo journalctl -fu docker.service Boot2Docker - /var/log/docker.log Debian GNU/Linux - /var/log/daemon.log CentOS - /var/log/daemon.log | grep docker CoreOS - journalctl -u docker.service Fedora - journalctl -u docker.service Red Hat Enterprise Linux Server - /var/log/messages | grep docker OpenSuSE - journalctl -u docker.service OSX - ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/log/d\u200c\u200bocker.log Windows - Get-EventLog -LogName Application -Source Docker -After (Get-Date).AddMinutes(-5) | Sort-Object Time","title":"Finding Docker Daemon Logs"},{"location":"docker-swarm-ucp-troubleshooting/#system-troubleshooting","text":"","title":"System Troubleshooting"},{"location":"docker-swarm-ucp-troubleshooting/#file-descriptors","text":"cat /proc/sys/fs/file-max cat /proc/sys/fs/file-nr [output] 1632 0 202648 where, 1632: currently allocated file descriptors 0: free allocated file descriptors 202648 : max file descriptors To update the limit ulimit -n 99999 sysctl -w fs.file-max=100000 docker run --ulimit nofile=90000:90000 <image-tag> To check open files lsof lsof | wc -l lsof | grep <pid> References Etcd and RethinkDB Troubleshooting: https://docs.docker.com/datacenter/ucp/2.2/guides/admin/monitor-and-troubleshoot/troubleshoot-configurations/#check-the-status-of-the-database Where are docker daemon logs, stackoverflow discussion https://stackoverflow.com/questions/30969435/where-is-the-docker-daemon-log?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa File Descriptors: https://www.netadmintools.com/art295.html","title":"File Descriptors"},{"location":"docker-swarm/","text":"Docker Swarm - Quick Dive With Swarm Mode ( Docker version 1.12) docker-machine create -d virtualbox master docker-machine create -d virtualbox node1 docker-machine create -d virtualbox node2 In window 1 docker-machine env master [execute the command to setup env ] docker swarm init --advertise-addr <IP_ADDRESS_OF_MASTER> docker node ls In window 2 docker-machine env node1 docker swarm join \\ --token <TOKEN> \\ <IP_ADDRESS_OF_MASTER> In window 3 docker-machine env node1 docker swarm join \\ --token <TOKEN> \\ <IP_ADDRESS_OF_MASTER> In window 1 docker node ls docker service create --replicas 1 --name helloworld alpine ping docker.com docker service ls docker service inspect --pretty helloworld docker service scale helloworld=5 docker service ps helloworld docker node ps node1 docker service rm helloworld","title":"Docker Swarm - Quick Dive"},{"location":"docker-swarm/#docker-swarm-quick-dive","text":"","title":"Docker Swarm - Quick Dive"},{"location":"docker-swarm/#with-swarm-mode-docker-version-112","text":"docker-machine create -d virtualbox master docker-machine create -d virtualbox node1 docker-machine create -d virtualbox node2","title":"With Swarm Mode ( Docker version 1.12)"},{"location":"docker-swarm/#in-window-1","text":"docker-machine env master [execute the command to setup env ] docker swarm init --advertise-addr <IP_ADDRESS_OF_MASTER> docker node ls","title":"In window 1"},{"location":"docker-swarm/#in-window-2","text":"docker-machine env node1 docker swarm join \\ --token <TOKEN> \\ <IP_ADDRESS_OF_MASTER>","title":"In window 2"},{"location":"docker-swarm/#in-window-3","text":"docker-machine env node1 docker swarm join \\ --token <TOKEN> \\ <IP_ADDRESS_OF_MASTER>","title":"In window 3"},{"location":"docker-swarm/#in-window-1_1","text":"docker node ls docker service create --replicas 1 --name helloworld alpine ping docker.com docker service ls docker service inspect --pretty helloworld docker service scale helloworld=5 docker service ps helloworld docker node ps node1 docker service rm helloworld","title":"In window 1"},{"location":"dockerizing-facebooc/","text":"Project : Build a docker image for Facebooc app Facebooc is a app written in C. Its a simple, web based social media application clone and used SQLite as a database backend. As a devops engineer, you have been tasked with building an image for facebooc app and publish it to docker hub registry. Approach 1: Building docker image for facebooc app manually on the host git clone https://github.com/schoolofdevops/facebooc.git docker container run -idt --name fb -p 16000:16000 ubuntu bash docker cp facebooc fb:/opt docker exec -it fb bash inside the container cd /opt/facebooc/ apt-get update apt-get install -yq build-essential make libsqlite3-dev sqlite3 make all bin/facebooc on the host docker diff fb docker container commit fb <docker_id>/facebooc:v1 docker login docker image push <docker_id>/facebooc:v1 Approach 2: Building image with Dockerfile Change into facebooc directory which containts the source code. This assumes you have already cloned the repo. If not, clone it from https://github.com/schoolofdevops/facebooc cd facebooc ls LICENSE Makefile README.md include src static templates Add/create Dockerfile the the same directory (facebooc) witht the following content, FROM ubuntu WORKDIR /opt/facebooc RUN apt-get update && \\ apt-get install -yq build-essential make git libsqlite3-dev sqlite3 COPY . /opt/facebooc RUN make all EXPOSE 16000 CMD \"bin/facebooc\" Build image using, docker build -t <docker_id>/facebooc:v2 . where, : your docker registry user/namespace. Replace this with the actual user validate docker image ls docker image history <docker_id>/facebooc:v2 docker container run -idt -P <docker_id>/facebooc:v2 docker ps Check by connecting to your host:port to validate if facebooc web application shows up. Once validated, tag and push docker image tag <docker_id>/facebooc:v2 <docker_id>/facebooc:latest docker login docker push <docker_id>/facebooc","title":"Dockerizing your own App"},{"location":"dockerizing-facebooc/#project-build-a-docker-image-for-facebooc-app","text":"Facebooc is a app written in C. Its a simple, web based social media application clone and used SQLite as a database backend. As a devops engineer, you have been tasked with building an image for facebooc app and publish it to docker hub registry.","title":"Project : Build a docker image for Facebooc app"},{"location":"dockerizing-facebooc/#approach-1-building-docker-image-for-facebooc-app-manually","text":"on the host git clone https://github.com/schoolofdevops/facebooc.git docker container run -idt --name fb -p 16000:16000 ubuntu bash docker cp facebooc fb:/opt docker exec -it fb bash inside the container cd /opt/facebooc/ apt-get update apt-get install -yq build-essential make libsqlite3-dev sqlite3 make all bin/facebooc on the host docker diff fb docker container commit fb <docker_id>/facebooc:v1 docker login docker image push <docker_id>/facebooc:v1","title":"Approach 1: Building docker image for facebooc app manually"},{"location":"dockerizing-facebooc/#approach-2-building-image-with-dockerfile","text":"Change into facebooc directory which containts the source code. This assumes you have already cloned the repo. If not, clone it from https://github.com/schoolofdevops/facebooc cd facebooc ls LICENSE Makefile README.md include src static templates Add/create Dockerfile the the same directory (facebooc) witht the following content, FROM ubuntu WORKDIR /opt/facebooc RUN apt-get update && \\ apt-get install -yq build-essential make git libsqlite3-dev sqlite3 COPY . /opt/facebooc RUN make all EXPOSE 16000 CMD \"bin/facebooc\" Build image using, docker build -t <docker_id>/facebooc:v2 . where, : your docker registry user/namespace. Replace this with the actual user validate docker image ls docker image history <docker_id>/facebooc:v2 docker container run -idt -P <docker_id>/facebooc:v2 docker ps Check by connecting to your host:port to validate if facebooc web application shows up. Once validated, tag and push docker image tag <docker_id>/facebooc:v2 <docker_id>/facebooc:latest docker login docker push <docker_id>/facebooc","title":"Approach 2: Building image with Dockerfile"},{"location":"dockerizing-voteapp/","text":"Project : Build a docker image for Instavote frontend vote app Voteapp is a app written in python. Its a simple, web based application which serves as a frontend for Instavote project. As a devops engineer, you have been tasked with building an image for vote app and publish it to docker hub registry. Approach 1: Building docker image for facebooc app manually on the host git clone https://github.com/schoolofdevops/vote docker container run -idt --name vote -p 8000:80 python:2.7-alpine sh cd vote docker cp . vote:/app docker exec -it vote sh inside the container cd /app pip install -r requirements.txt gunicorn app:app -b 0.0.0.0:80 Validate by accessing http://IPADDRESS:8000 on the host docker diff fb docker container commit vote <docker_id>/vote:v1 docker login docker image push <docker_id>/vote:v1 Approach 2: Building image with Dockerfile Change into facebooc directory which containts the source code. This assumes you have already cloned the repo. If not, clone it from https://github.com/schoolofdevops/facebooc cd vote ls app.py requirements.txt static templates Add/create Dockerfile the the same directory (vote) with the following content, # Using official python runtime base image FROM python:2.7-alpine # Set the application directory WORKDIR /app # Install our requirements.txt ADD requirements.txt /app/requirements.txt RUN pip install -r requirements.txt # Copy our code from the current folder to /app inside the container ADD . /app # Make port 80 available for links and/or publish EXPOSE 80 # Define our command to be run when launching the container CMD gunicorn app:app -b 0.0.0.0:80 --log-file - --access-logfile - --workers 4 --keep-alive 0 Build image using, docker build -t <docker_id>/vote:v2 . where, : your docker registry user/namespace. Replace this with the actual user validate docker image ls docker image history <docker_id>/vote:v2 docker image history <docker_id>/vote:v1 docker container run -idt -P <docker_id>/vote:v2 docker ps Check by connecting to your host:port to validate if vote web application shows up. Once validated, tag and push docker image tag <docker_id>/vote:v2 <docker_id>/vote:latest docker login docker push <docker_id>/vote","title":"Dockerizing voteapp"},{"location":"dockerizing-voteapp/#project-build-a-docker-image-for-instavote-frontend-vote-app","text":"Voteapp is a app written in python. Its a simple, web based application which serves as a frontend for Instavote project. As a devops engineer, you have been tasked with building an image for vote app and publish it to docker hub registry.","title":"Project : Build a docker image for Instavote frontend vote app"},{"location":"dockerizing-voteapp/#approach-1-building-docker-image-for-facebooc-app-manually","text":"on the host git clone https://github.com/schoolofdevops/vote docker container run -idt --name vote -p 8000:80 python:2.7-alpine sh cd vote docker cp . vote:/app docker exec -it vote sh inside the container cd /app pip install -r requirements.txt gunicorn app:app -b 0.0.0.0:80 Validate by accessing http://IPADDRESS:8000 on the host docker diff fb docker container commit vote <docker_id>/vote:v1 docker login docker image push <docker_id>/vote:v1","title":"Approach 1: Building docker image for facebooc app manually"},{"location":"dockerizing-voteapp/#approach-2-building-image-with-dockerfile","text":"Change into facebooc directory which containts the source code. This assumes you have already cloned the repo. If not, clone it from https://github.com/schoolofdevops/facebooc cd vote ls app.py requirements.txt static templates Add/create Dockerfile the the same directory (vote) with the following content, # Using official python runtime base image FROM python:2.7-alpine # Set the application directory WORKDIR /app # Install our requirements.txt ADD requirements.txt /app/requirements.txt RUN pip install -r requirements.txt # Copy our code from the current folder to /app inside the container ADD . /app # Make port 80 available for links and/or publish EXPOSE 80 # Define our command to be run when launching the container CMD gunicorn app:app -b 0.0.0.0:80 --log-file - --access-logfile - --workers 4 --keep-alive 0 Build image using, docker build -t <docker_id>/vote:v2 . where, : your docker registry user/namespace. Replace this with the actual user validate docker image ls docker image history <docker_id>/vote:v2 docker image history <docker_id>/vote:v1 docker container run -idt -P <docker_id>/vote:v2 docker ps Check by connecting to your host:port to validate if vote web application shows up. Once validated, tag and push docker image tag <docker_id>/vote:v2 <docker_id>/vote:latest docker login docker push <docker_id>/vote","title":"Approach 2: Building image with Dockerfile"},{"location":"markdown_cheatsheet/","text":"{pagebreak} Part Section Sub Section Sub Sub Section Sub Sub Sub Section This is the first para This is the 2nd para separated by a line This is the 3rd para separated by 2 spaces at the end of line before c> This is a centered text italic bold bold-italic _underlined _ Numbered list item 1 Numbered list item 2 para between list item 4 indents, blank like after prev item, num seq continues Numbered list item 3 Bullet list item 1 Bullet list item 2 Sub Item 2.1 Sub Sub item 2.1.1 Bullet list item 3 Header : description of the header This is a Block Quote Block Quote gets indented block quote inside a block quote Text Blocks Asides A> ## This is an aside A> A> This gets printed in a block Warnings W> ## This is a Warning W> W> Always wear the seat belt Tips T> ## This is a tip T> T> Bet for number 5 on that table Errors E> ## This prints an error E> E> Oops ! You just barged into ladies toilet Information I> ## This is to print info I> I> For your eyes only Questions Q> ## This prints questions Q> Q> What came first, chicken or the engg? Discussions D> ## This is for Discussions D> D> Lets talk about Life of Pie Exercises X> ## This is for exercises X> X> 10 mins on treadmill X> followed by two sets of pushups Writing Code Method 1 : 4 space indentation echo \"this starts with 4 indents\" ls -l cat /etc/issue uptime Method 2: 8 tildes echo \"this has 8 tildes on the top and bottom\" echo \"8 tildes is a best practice, but any no. will do \" uptime ls -l cat /etc/issue Method 3 : include code from file <<(code/sample_code.sh) << sample_code_with_title Method 4 : lp-code style {title=\"Listing \", lang=html, linenos=off} echo \"this is autocompleted by leanpub plugin for atom\" uptime df -h ls -l Method 5: back ticks `` echo \"this is a short code snippet\" Footnotes This is a text para and I am writing a footnote[^tag1] here Later on I add the footnote as [^tag1]: description about the footnote. Should contain blank line before and after Crosslinks This is a cross link to {##Sub Section} Table header1 header2 r1-c1 r1-c2 r2-c1 r2-c2 r3-c1 r3-c2 Header One Header Two Item One Item Two","title":"Markdown cheatsheet"},{"location":"markdown_cheatsheet/#part","text":"","title":"Part"},{"location":"markdown_cheatsheet/#section","text":"","title":"Section"},{"location":"markdown_cheatsheet/#sub-section","text":"","title":"Sub Section"},{"location":"markdown_cheatsheet/#sub-sub-section","text":"","title":"Sub Sub Section"},{"location":"markdown_cheatsheet/#sub-sub-sub-section","text":"This is the first para This is the 2nd para separated by a line This is the 3rd para separated by 2 spaces at the end of line before c> This is a centered text italic bold bold-italic _underlined _ Numbered list item 1 Numbered list item 2 para between list item 4 indents, blank like after prev item, num seq continues Numbered list item 3 Bullet list item 1 Bullet list item 2 Sub Item 2.1 Sub Sub item 2.1.1 Bullet list item 3 Header : description of the header This is a Block Quote Block Quote gets indented block quote inside a block quote Text Blocks Asides A> ## This is an aside A> A> This gets printed in a block Warnings W> ## This is a Warning W> W> Always wear the seat belt Tips T> ## This is a tip T> T> Bet for number 5 on that table Errors E> ## This prints an error E> E> Oops ! You just barged into ladies toilet Information I> ## This is to print info I> I> For your eyes only Questions Q> ## This prints questions Q> Q> What came first, chicken or the engg? Discussions D> ## This is for Discussions D> D> Lets talk about Life of Pie Exercises X> ## This is for exercises X> X> 10 mins on treadmill X> followed by two sets of pushups Writing Code Method 1 : 4 space indentation echo \"this starts with 4 indents\" ls -l cat /etc/issue uptime Method 2: 8 tildes echo \"this has 8 tildes on the top and bottom\" echo \"8 tildes is a best practice, but any no. will do \" uptime ls -l cat /etc/issue Method 3 : include code from file <<(code/sample_code.sh) << sample_code_with_title Method 4 : lp-code style {title=\"Listing \", lang=html, linenos=off} echo \"this is autocompleted by leanpub plugin for atom\" uptime df -h ls -l Method 5: back ticks `` echo \"this is a short code snippet\" Footnotes This is a text para and I am writing a footnote[^tag1] here Later on I add the footnote as [^tag1]: description about the footnote. Should contain blank line before and after Crosslinks This is a cross link to {##Sub Section} Table header1 header2 r1-c1 r1-c2 r2-c1 r2-c2 r3-c1 r3-c2 Header One Header Two Item One Item Two","title":"Sub Sub Sub Section"},{"location":"persistent-volumes/","text":"Lab: Persistent Volumes with Docker Types of volumes automatic volumes named volumes volume binding Automatic Volumes docker container run -idt --name vt01 -v /var/lib/mysql alpine sh docker inspect vt01 | grep -i mounts -A 10 Named volumes docker container run -idt --name vt02 -v db-data:/var/lib/mysql alpine sh docker inspect vt02 | grep -i mounts -A 10 Volume binding mkdir /root/sysfoo docker container run -idt --name vt03 -v /root/sysfoo:/var/lib/mysql alpine sh docker inspect vt03 | grep -i mounts -A 10 Sharing files between host and the container ls /root/sysfoo/ touch /root/sysfoo/file1 docker exec -it vt03 sh ls sysfoo/","title":"Docker Data Persistence"},{"location":"persistent-volumes/#lab-persistent-volumes-with-docker","text":"","title":"Lab: Persistent Volumes with Docker"},{"location":"persistent-volumes/#types-of-volumes","text":"automatic volumes named volumes volume binding Automatic Volumes docker container run -idt --name vt01 -v /var/lib/mysql alpine sh docker inspect vt01 | grep -i mounts -A 10 Named volumes docker container run -idt --name vt02 -v db-data:/var/lib/mysql alpine sh docker inspect vt02 | grep -i mounts -A 10 Volume binding mkdir /root/sysfoo docker container run -idt --name vt03 -v /root/sysfoo:/var/lib/mysql alpine sh docker inspect vt03 | grep -i mounts -A 10 Sharing files between host and the container ls /root/sysfoo/ touch /root/sysfoo/file1 docker exec -it vt03 sh ls sysfoo/","title":"Types of volumes"},{"location":"swarm-networking-deepdive/","text":"SWARM Networking Deep Dive In this module, we are going to set on a interesting journey of how SWARM netwoking functions under the hood. We will delving deeper in the world of bridges, vxlans, overlays, underlays, kernel ipvs and follow the journey of a packet in a swarm cluster. We will also be looking into how docker leverages iptables and ipvs, both kernel features, to implement the service discovery and load balancing. Installing pre reqs Install bridge utils apt-get install bridge-utils Examine the networks before setting up Swarm brctl show bridge name bridge id STP enabled interfaces docker0 8000.024268987cd3 no docker network ls NETWORK ID NAME DRIVER SCOPE 896388d51d18 bridge bridge local 3e3e8fec9527 host host local 385a6e374d9d none null local Examine the network configurations created by SWARM List the networks docker network ls NETWORK ID NAME DRIVER SCOPE 9b3cdad15a64 bridge bridge local 71ad6ab6c0fb docker_gwbridge bridge local 6d42f614ce37 host host local lpq3tzoevynh ingress overlay swarm ce30767f4305 none null local where, docker_gwbridge : bridge network created by swarm to connect containers to host and outside world ingress: overlay network created by swarm for external service discovery, load balancing with routing mesh Examine the overlay vxlan inmplemntation Inspect networks docker network inspect docker_gwbridge [output] \"Containers\": { \"ingress-sbox\": { \"Name\": \"gateway_ingress-sbox\", \"EndpointID\": \"b735335b753af4222fa253ba8496fe5a9bff10f8ddc698bd938d2b3e10780d54\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.18.0.2/16\", \"IPv6Address\": \"\" } }, where, ingress-sbox : sandbox created with network namespace and configs, purely for service discovery and load balancing EndpointID : endpoing created (veth pair) in ingress-sbox e.g. eth0 inside this network namespace docker network inspect ingress [output] \"Containers\": { \"ingress-sbox\": { \"Name\": \"ingress-endpoint\", \"EndpointID\": \"a187751fda1c95b0f9c47bfe5d4104cf5195a839fef588bc7e3b02da5972ca7a\", \"MacAddress\": \"02:42:0a:ff:00:02\", \"IPv4Address\": \"10.255.0.2/16\", \"IPv6Address\": \"\" } }, \"Options\": { \"com.docker.network.driver.overlay.vxlanid_list\": \"4096\" }, \"Labels\": {}, \"Peers\": [ { \"Name\": \"02dddbfc3e9a\", \"IP\": \"159.65.167.88\" }, { \"Name\": \"96473fed4b7c\", \"IP\": \"159.89.42.230\" }, { \"Name\": \"c92920c69b92\", \"IP\": \"159.89.41.130\" } ] } where, ingress-sbox : sandbox created with network namespace and configs, purely for service discovery and load balancing EndpointID : endpoing created (veth pair) in ingress-sbox Peers : nodes participating in this overlay 4096 : VXLAN ID We will look inside the ingress-sbox namespaces as later part of this tutorial. Interfaces and bridges ifconfig brctl show [output] brctl show bridge name bridge id STP enabled interfaces docker0 8000.02425dcabce4 no veth3850215 docker_gwbridge 8000.0242105642b6 no veth4dae0de ov-001000-wo0i1 8000.1e8f6f3278a0 no vethc978c4b vx-001000-wo0i1 Note down the vx-001000-wo0i1 id. To check more information use the following command. [ Replace the command with your VXLAN ID ] ip -d link show vx-001000-wo0i1 Show forwarding table bridge fdb show dev vx-001000-wo0i1 [output] 5e:20:18:b1:1d:0e vlan 0 permanent 02:42:0a:ff:00:03 dst 159.89.39.105 self permanent 02:42:0a:ff:00:04 dst 165.227.64.215 self permanent where, 5e:20:18:b1:1d:0e => mac of the current host 02:42:2c:32:94:4e => mac id of ingress_box endpoint for ingress network on host with ip 159.89.39.105 02:42:b2:0d:24:f8 => mac id of ingress_box endpoint for ingress network on host with ip 165.227.64.215 Examine the traffic Traffic on 2377/tcp : Cluster management communication tcpdump -v -i eth0 port 2377 Inter node gossip tcpdump -v -i eth0 port 7946 Data plan traffic on overlay tcpdump -v -i eth0 udp and port 4789 Creating overlay networks docker network create -d overlay mynet0 docker network ls docker network inspect mynet0 Examine the options, its missing encrypted flag \"ConfigOnly\": false, \"Containers\": null, \"Options\": { \"com.docker.network.driver.overlay.vxlanid_list\": \"4097\" }, \"Labels\": null where, 4097 : vnid of this VXLAN docker network create --opt encrypted -d overlay vote docker network ls docker network inspect vote this time, encryption is enabled \"Containers\": null, \"Options\": { \"com.docker.network.driver.overlay.vxlanid_list\": \"4098\", \"encrypted\": \"\" }, \"Labels\": null } Try This Observe the following by listing networks on all nodes, docker network ls all manager nodes have the new overlay network worker nodes will create it on need basis, only if there is a task running on that node Lets learn what all is created with this overlay network, ifconfig brctl show Launch Service with overlay network docker service ls docker service create --name redis --network vote --replicas=2 redis:alpine [output] 8mxs1phssydpwi23teifpqcwr overall progress: 2 out of 2 tasks 1/2: running [==================================================>] 2/2: running [==================================================>] verify: Service converged Check network on all nodes. It would be created only on selective nodes where tasks are scheduled docker network ls docker network inspect vote docker ps 4ea9c75179c3 redis:alpine \"docker-entrypoint.s\u2026\" About a minute ago Up About a minute 6379/tcp redis.2.pbyb0o2e60gm1ozwc3wz9f7ou Correlate interfaces and trace it Inside the container docker exec 4ea9c75179c3 ip link 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 16: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1424 qdisc noqueue state UP link/ether 02:42:0a:00:00:07 brd ff:ff:ff:ff:ff:ff 18: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:12:00:03 brd ff:ff:ff:ff:ff:ff and on the host root@swarm-01:/var/run# ip link 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 9e:28:4c:8a:bf:5d brd ff:ff:ff:ff:ff:ff 3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default link/ether 02:42:5d:ca:bc:e4 brd ff:ff:ff:ff:ff:ff 7: ov-001000-wo0i1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP mode DEFAULT group default link/ether 1e:8f:6f:32:78:a0 brd ff:ff:ff:ff:ff:ff 8: vx-001000-wo0i1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master ov-001000-wo0i1 state UNKNOWN mode DEFAULT group default link/ether 5e:20:18:b1:1d:0e brd ff:ff:ff:ff:ff:ff 10: vethc978c4b: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master ov-001000-wo0i1 state UP mode DEFAULT group default link/ether 1e:8f:6f:32:78:a0 brd ff:ff:ff:ff:ff:ff 11: docker_gwbridge: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:10:56:42:b6 brd ff:ff:ff:ff:ff:ff 13: veth4dae0de: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker_gwbridge state UP mode DEFAULT group default link/ether 26:ea:d2:47:25:0d brd ff:ff:ff:ff:ff:ff 14: ov-001001-7672d: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1424 qdisc noqueue state UP mode DEFAULT group default link/ether 42:a3:4b:f3:ca:05 brd ff:ff:ff:ff:ff:ff 15: vx-001001-7672d: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1424 qdisc noqueue master ov-001001-7672d state UNKNOWN mode DEFAULT group default link/ether 42:a3:4b:f3:ca:05 brd ff:ff:ff:ff:ff:ff 17: veth4dd295a: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1424 qdisc noqueue master ov-001001-7672d state UP mode DEFAULT group default link/ether ba:83:a3:3c:73:b1 brd ff:ff:ff:ff:ff:ff 19: veth78165ba: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker_gwbridge state UP mode DEFAULT group default link/ether 76:9a:35:13:b3:64 brd ff:ff:ff:ff:ff:ff veth Pairs 16: eth0 <===> 17: veth4dd295a (Overlay ov-001001-7672d) 18: eth1 <===> 19: veth78165ba (docker_gwbridge) Show forwarding table for this overlay vtep brctl show bridge fdb show dev vx-001001-7672d [output] 42:a3:4b:f3:ca:05 vlan 0 permanent 02:42:0a:00:00:06 dst 159.89.39.105 self permanent Where, 02:42:0a:00:00:06 should be the mac id of the container on the other hosts e.g. on swarm-2 root@swarm-02:~# docker exec 92ee739ecdca ip lin 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 16: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1424 qdisc noqueue state UP link/ether 02:42:0a:00:00:06 brd ff:ff:ff:ff:ff:ff root@swarm-02:~# ip link 17: veth353ea84: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1424 qdisc noqueue master ov-001001-7672d state UP mode DEFAULT group default link/ether 6e:2a:bb:f1:52:2a brd ff:ff:ff:ff:ff:ff Task: Do the same from the other end. Check the fdb for vxlan interface on swarm-02 and correlate it with the mac id of a container running on swarm-01 Launch worker service in same overlay docker service create --name worker --network vote schoolofdevops/worker Should get launched on the third node as the default scheduling algorithm is to sread the load evenly. Now on node1 and node2 check the fdb again, should see a new vtep endpoint e.g. [replace vx-001001-7672d with the id of the vxlan interface created for this overlayn/w, get it by using brctl show ] bridge fdb show dev vx-001001-7672d 22:ba:86:43:71:27 vlan 0 permanent 02:42:0a:00:00:07 dst 159.65.161.208 self permanent 02:42:0a:00:00:09 dst 165.227.64.215 self permanent Scale redis service docker service scale redis=5 Examine the fdb again bridge fdb show dev vx-001001-7672d 42:a3:4b:f3:ca:05 vlan 0 permanent 02:42:0a:00:00:09 vlan 0 02:42:0a:00:00:06 dst 159.89.39.105 self permanent 02:42:0a:00:00:09 dst 165.227.64.215 self permanent 02:42:0a:00:00:0a dst 165.227.64.215 self permanent 02:42:0a:00:00:0b dst 159.89.39.105 self permanent where, the table shows entries for every other Underlying VXLAN service and traffic port 4789 is reservered for vxlan. Packets will have headers with this. netstat -pan | grep 4789 To see the packets going through the vxlan interface brctl show tcpdump -i ov-001001-7672d Internal Load Balancing connect to one of the redis instances on one of the nodes docker ps docker run --rm -it --net container:0b0309771045 --privileged nicolaka/netshoot Verify redirect to ipvs iptables -nvL -t nat Chain POSTROUTING (policy ACCEPT 85 packets, 5426 bytes) pkts bytes target prot opt in out source destination 59 3712 DOCKER_POSTROUTING all -- * * 0.0.0.0/0 127.0.0.11 3 180 SNAT all -- * * 0.0.0.0/0 10.0.0.0/24 ipvs to:10.0.0.11 where, ipvs to:10.0.0.11 . : is routing the traffic to ipvs, running on the same container check the mangle markers iptables -nvL -t mangle Chain OUTPUT (policy ACCEPT 864 packets, 62611 bytes) pkts bytes target prot opt in out source destination 0 0 MARK all -- * * 0.0.0.0/0 10.0.0.5 MARK set 0x100 168 14112 MARK all -- * * 0.0.0.0/0 10.0.0.8 MARK set 0x101 26 1757 MARK all -- * * 0.0.0.0/0 10.0.0.13 MARK set 0x103 where, 10.0.0.5 is s VIP for service xyz . 0x100is a HEX for 256. to check where this is redirecting, look at the ipvs rules # ipvsadm IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -> RemoteAddress:Port Forward Weight ActiveConn InActConn FWM 256 rr -> redis.1.m911y2nt3wy0qo5x8i9p Masq 1 0 0 -> redis.2.pbyb0o2e60gm1ozwc3wz Masq 1 0 0 -> e89fbe75fb1a.vote:0 Masq 1 0 0 -> 0b0309771045:0 Masq 1 0 0 -> redis.5.oag34plamnmptoby9b0y Masq 1 0 0 FWM 257 rr -> worker.1.lhxvgjgn5k6soaksifz Masq 1 0 0 FWM 259 rr -> vote.1.hhv9l10vb5yocxmkbzzdv Masq 1 0 0 -> vote.2.h1iwvk5t8hr9pc6mpqsxk Masq 1 0 0 here the following is doing a RR load balancing across 5 nodes FWM 256 rr -> redis.1.m911y2nt3wy0qo5x8i9p Masq 1 0 0 -> redis.2.pbyb0o2e60gm1ozwc3wz Masq 1 0 0 -> e89fbe75fb1a.vote:0 Masq 1 0 0 -> 0b0309771045:0 Masq 1 0 0 -> redis.5.oag34plamnmptoby9b0y Masq 1 0 0 Port Publishing, Routing Mesh, Ingress Network and External Service Discovery docker network ls docker network inspect ingress where, Peers : shows all the hosts which are part of this ingress (note the peers and corraborate) Containers : shows ingress-sbox namespace (its not a containers, just a namespace, has one interface in gwbridge, another ingress) Examine the ingress-sbox namespace Learn about netshoot utility at https://github.com/nicolaka/netshoot Launch netshoot container, and connect to ingress-sbox using nsenter. docker run -it --rm -v /var/run/docker/netns:/netns --privileged=true nicolaka/netshoot nsenter --net=/netns/ingress_sbox sh From inside netshoot container, ``` ifconfig ip link show [output] 1: lo: mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 9: eth0: mtu 1450 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:0a:ff:00:02 brd ff:ff:ff:ff:ff:ff 12: eth1: mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff ``` On the host ip link show [output] 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 92:20:8a:88:b6:e8 brd ff:ff:ff:ff:ff:ff 3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:68:98:7c:d3 brd ff:ff:ff:ff:ff:ff 7: ov-001000-lpq3t: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP mode DEFAULT group default link/ether 8a:17:46:93:46:94 brd ff:ff:ff:ff:ff:ff 8: vx-001000-lpq3t: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master ov-001000-lpq3t state UNKNOWN mode DEFAULT group default link/ether 8a:17:46:93:46:94 brd ff:ff:ff:ff:ff:ff 10: veth28c87ba: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master ov-001000-lpq3t state UP mode DEFAULT group default link/ether 8a:24:17:29:46:a7 brd ff:ff:ff:ff:ff:ff 11: docker_gwbridge: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:a0:ca:1d:96 brd ff:ff:ff:ff:ff:ff 13: veth0740008: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker_gwbridge state UP mode DEFAULT group default link/ether aa:b8:35:c2:97:74 brd ff:ff:ff:ff:ff:ff 19: veth97a403d: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default link/ether 8a:80:a4:17:3b:2d brd ff:ff:ff:ff:ff:ff If you compare two outputs above, 9 <=====> 10 : ingress network 12 <=====> 13 : docker_gwbridge network These are then further bridged. Examine the bridges in the next part. Create a container which is part of this ingress docker service create --name vote --network vote --publish 80 --replicas=2 schoolofdevops/vote docker ps [output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 85642d7fa2f4 schoolofdevops/vote:latest \"gunicorn app:app -b\ufffd\" 28 seconds ago Up 27 seconds 80/tcp vote.1.hhv9l10vb5yocxmkbzzdvtmj2 64f05b29e559 redis:alpine \"docker-entrypoint.s\ufffd\" 29 minutes ago Up 29 minutes 6379/tcp redis.5.oag34plamnmptoby9b0yuaooi 4ea9c75179c3 redis:alpine \"docker-entrypoint.s\ufffd\" About an hour ago Up About an hour 6379/tcp redis.2.pbyb0o2e60gm1ozwc3wz9f7ou Connect to container and examine docker exec 85642d7fa2f4 ifconfig docker exec 85642d7fa2f4 ip link show docker exec 85642d7fa2f4 netstat -nr Correlate it with the host veth pair ip link show brctl show docker network ls eth0 => ingress eth1 => gwbridge eth2 => overlay for apps Service Networking and Routing Mesh For external facing ingress connnetiion, service routing works this way, ingress ==> gwbridge ==> ingress-sbox (its just a n/w namespae not a container) ==> ipvs ==> underlay Check iptable rules on the host iptables -nvL -t nat [output] ... Chain DOCKER-INGRESS (2 references) pkts bytes target prot opt in out source destination 0 0 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:30000 to:172.18.0.2:30000 31 1744 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 where, tcp dpt:30000 to:172.18.0.2:30000 => is forwarding the traffic received on 30000 port to 172.18.0.2:30000. Here 172.18.0.2 belongs to ingress_sbox so whatever happens next is inside there... Connecting to ingress-sbox docker run -it --rm -v /var/run/docker/netns:/var/run/docker/netns --privileged=true nicolaka/netshoot nsenter --net=/var/run/docker/netns/ingress_sbox sh alternately docker run -it --rm -v /var/run/docker/netns:/netns --privileged=true nicolaka/netshoot nsenter --net=/netns/ingress_sbox sh and then iptables -nvL -t mangle Chain PREROUTING (policy ACCEPT 16 packets, 1888 bytes) pkts bytes target prot opt in out source destination 0 0 MARK tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:30000 MARK set 0x102 where, iptables is setting MARK to 0x102 for anything that comes in on 30000 port. 0x102 is hex value and can be translated into integer from here https://www.binaryhexconverter.com/hex-to-decimal-converter e.g. 0x102 = 258 Now check the rules for above mark with ipvs ipvsadm [output] IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -> RemoteAddress:Port Forward Weight ActiveConn InActConn FWM 258 rr -> 10.255.0.6:0 Masq 1 0 0 -> 10.255.0.7:0 Masq 1 0 0 This is where the decision is made as to where this packet goes. Since ipvs uses round robin algorithm, one of these ips are selected and then packet is sent over the ingress overlay network. Finally, To see the traffic on ingress network on node2 tcpdump -i eth0 udp and port 4789 tcpdump -i eth0 esp Additional Commands tail -f syslog tcpdump -i eth0 udp and port 4789 tcpdump -i eth0 esp ip addr ip link iptables -t nat -nvL To see namespaces on the docker host cd /var/run ln -s /var/run/docker/netns netns ip netns docker network ls [company network and ns ids] References CNM and Libnetwork https://github.com/docker/libnetwork/blob/master/docs/design.md How VXLANs work ? https://youtu.be/Jqm_4TMmQz8?t=32s (watch from 00.32 to xx.xx) https://www.youtube.com/watch?v=YNqKDI_bnPM Overlay Tutorial https://neuvector.com/network-security/docker-swarm-container-networking/ Docker Networking Tutorial - Learning by Practicing https://www.securitynik.com/2016/12/docker-networking-internals-container.html Swarm networks https://docs.docker.com/v17.09/engine/swarm/networking/ Ip cheatsheet https://access.redhat.com/sites/default/files/attachments/rh_ip_command_cheatsheet_1214_jcs_print.pdf Overlay issues https://github.com/moby/moby/issues/30820 Network Troubleshooting https://success.docker.com/article/troubleshooting-container-networking Connect Service to Multiple Networks: https://www.slideshare.net/SreenivasMakam/docker-networking-common-issues-and-troubleshooting-techniques","title":"SWARM Networking Deep Dive"},{"location":"swarm-networking-deepdive/#swarm-networking-deep-dive","text":"In this module, we are going to set on a interesting journey of how SWARM netwoking functions under the hood. We will delving deeper in the world of bridges, vxlans, overlays, underlays, kernel ipvs and follow the journey of a packet in a swarm cluster. We will also be looking into how docker leverages iptables and ipvs, both kernel features, to implement the service discovery and load balancing.","title":"SWARM Networking Deep Dive"},{"location":"swarm-networking-deepdive/#installing-pre-reqs","text":"Install bridge utils apt-get install bridge-utils","title":"Installing pre reqs"},{"location":"swarm-networking-deepdive/#examine-the-networks-before-setting-up-swarm","text":"brctl show bridge name bridge id STP enabled interfaces docker0 8000.024268987cd3 no docker network ls NETWORK ID NAME DRIVER SCOPE 896388d51d18 bridge bridge local 3e3e8fec9527 host host local 385a6e374d9d none null local","title":"Examine the networks before setting up Swarm"},{"location":"swarm-networking-deepdive/#examine-the-network-configurations-created-by-swarm","text":"List the networks docker network ls NETWORK ID NAME DRIVER SCOPE 9b3cdad15a64 bridge bridge local 71ad6ab6c0fb docker_gwbridge bridge local 6d42f614ce37 host host local lpq3tzoevynh ingress overlay swarm ce30767f4305 none null local where, docker_gwbridge : bridge network created by swarm to connect containers to host and outside world ingress: overlay network created by swarm for external service discovery, load balancing with routing mesh Examine the overlay vxlan inmplemntation","title":"Examine the network configurations created by SWARM"},{"location":"swarm-networking-deepdive/#inspect-networks","text":"docker network inspect docker_gwbridge [output] \"Containers\": { \"ingress-sbox\": { \"Name\": \"gateway_ingress-sbox\", \"EndpointID\": \"b735335b753af4222fa253ba8496fe5a9bff10f8ddc698bd938d2b3e10780d54\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.18.0.2/16\", \"IPv6Address\": \"\" } }, where, ingress-sbox : sandbox created with network namespace and configs, purely for service discovery and load balancing EndpointID : endpoing created (veth pair) in ingress-sbox e.g. eth0 inside this network namespace docker network inspect ingress [output] \"Containers\": { \"ingress-sbox\": { \"Name\": \"ingress-endpoint\", \"EndpointID\": \"a187751fda1c95b0f9c47bfe5d4104cf5195a839fef588bc7e3b02da5972ca7a\", \"MacAddress\": \"02:42:0a:ff:00:02\", \"IPv4Address\": \"10.255.0.2/16\", \"IPv6Address\": \"\" } }, \"Options\": { \"com.docker.network.driver.overlay.vxlanid_list\": \"4096\" }, \"Labels\": {}, \"Peers\": [ { \"Name\": \"02dddbfc3e9a\", \"IP\": \"159.65.167.88\" }, { \"Name\": \"96473fed4b7c\", \"IP\": \"159.89.42.230\" }, { \"Name\": \"c92920c69b92\", \"IP\": \"159.89.41.130\" } ] } where, ingress-sbox : sandbox created with network namespace and configs, purely for service discovery and load balancing EndpointID : endpoing created (veth pair) in ingress-sbox Peers : nodes participating in this overlay 4096 : VXLAN ID We will look inside the ingress-sbox namespaces as later part of this tutorial.","title":"Inspect networks"},{"location":"swarm-networking-deepdive/#interfaces-and-bridges","text":"ifconfig brctl show [output] brctl show bridge name bridge id STP enabled interfaces docker0 8000.02425dcabce4 no veth3850215 docker_gwbridge 8000.0242105642b6 no veth4dae0de ov-001000-wo0i1 8000.1e8f6f3278a0 no vethc978c4b vx-001000-wo0i1 Note down the vx-001000-wo0i1 id. To check more information use the following command. [ Replace the command with your VXLAN ID ] ip -d link show vx-001000-wo0i1 Show forwarding table bridge fdb show dev vx-001000-wo0i1 [output] 5e:20:18:b1:1d:0e vlan 0 permanent 02:42:0a:ff:00:03 dst 159.89.39.105 self permanent 02:42:0a:ff:00:04 dst 165.227.64.215 self permanent where, 5e:20:18:b1:1d:0e => mac of the current host 02:42:2c:32:94:4e => mac id of ingress_box endpoint for ingress network on host with ip 159.89.39.105 02:42:b2:0d:24:f8 => mac id of ingress_box endpoint for ingress network on host with ip 165.227.64.215","title":"Interfaces and bridges"},{"location":"swarm-networking-deepdive/#examine-the-traffic","text":"Traffic on 2377/tcp : Cluster management communication tcpdump -v -i eth0 port 2377 Inter node gossip tcpdump -v -i eth0 port 7946 Data plan traffic on overlay tcpdump -v -i eth0 udp and port 4789","title":"Examine the traffic"},{"location":"swarm-networking-deepdive/#creating-overlay-networks","text":"docker network create -d overlay mynet0 docker network ls docker network inspect mynet0 Examine the options, its missing encrypted flag \"ConfigOnly\": false, \"Containers\": null, \"Options\": { \"com.docker.network.driver.overlay.vxlanid_list\": \"4097\" }, \"Labels\": null where, 4097 : vnid of this VXLAN docker network create --opt encrypted -d overlay vote docker network ls docker network inspect vote this time, encryption is enabled \"Containers\": null, \"Options\": { \"com.docker.network.driver.overlay.vxlanid_list\": \"4098\", \"encrypted\": \"\" }, \"Labels\": null } Try This Observe the following by listing networks on all nodes, docker network ls all manager nodes have the new overlay network worker nodes will create it on need basis, only if there is a task running on that node Lets learn what all is created with this overlay network, ifconfig brctl show","title":"Creating overlay networks"},{"location":"swarm-networking-deepdive/#launch-service-with-overlay-network","text":"docker service ls docker service create --name redis --network vote --replicas=2 redis:alpine [output] 8mxs1phssydpwi23teifpqcwr overall progress: 2 out of 2 tasks 1/2: running [==================================================>] 2/2: running [==================================================>] verify: Service converged Check network on all nodes. It would be created only on selective nodes where tasks are scheduled docker network ls docker network inspect vote docker ps 4ea9c75179c3 redis:alpine \"docker-entrypoint.s\u2026\" About a minute ago Up About a minute 6379/tcp redis.2.pbyb0o2e60gm1ozwc3wz9f7ou Correlate interfaces and trace it Inside the container docker exec 4ea9c75179c3 ip link 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 16: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1424 qdisc noqueue state UP link/ether 02:42:0a:00:00:07 brd ff:ff:ff:ff:ff:ff 18: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:12:00:03 brd ff:ff:ff:ff:ff:ff and on the host root@swarm-01:/var/run# ip link 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 9e:28:4c:8a:bf:5d brd ff:ff:ff:ff:ff:ff 3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default link/ether 02:42:5d:ca:bc:e4 brd ff:ff:ff:ff:ff:ff 7: ov-001000-wo0i1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP mode DEFAULT group default link/ether 1e:8f:6f:32:78:a0 brd ff:ff:ff:ff:ff:ff 8: vx-001000-wo0i1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master ov-001000-wo0i1 state UNKNOWN mode DEFAULT group default link/ether 5e:20:18:b1:1d:0e brd ff:ff:ff:ff:ff:ff 10: vethc978c4b: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master ov-001000-wo0i1 state UP mode DEFAULT group default link/ether 1e:8f:6f:32:78:a0 brd ff:ff:ff:ff:ff:ff 11: docker_gwbridge: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:10:56:42:b6 brd ff:ff:ff:ff:ff:ff 13: veth4dae0de: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker_gwbridge state UP mode DEFAULT group default link/ether 26:ea:d2:47:25:0d brd ff:ff:ff:ff:ff:ff 14: ov-001001-7672d: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1424 qdisc noqueue state UP mode DEFAULT group default link/ether 42:a3:4b:f3:ca:05 brd ff:ff:ff:ff:ff:ff 15: vx-001001-7672d: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1424 qdisc noqueue master ov-001001-7672d state UNKNOWN mode DEFAULT group default link/ether 42:a3:4b:f3:ca:05 brd ff:ff:ff:ff:ff:ff 17: veth4dd295a: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1424 qdisc noqueue master ov-001001-7672d state UP mode DEFAULT group default link/ether ba:83:a3:3c:73:b1 brd ff:ff:ff:ff:ff:ff 19: veth78165ba: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker_gwbridge state UP mode DEFAULT group default link/ether 76:9a:35:13:b3:64 brd ff:ff:ff:ff:ff:ff veth Pairs 16: eth0 <===> 17: veth4dd295a (Overlay ov-001001-7672d) 18: eth1 <===> 19: veth78165ba (docker_gwbridge) Show forwarding table for this overlay vtep brctl show bridge fdb show dev vx-001001-7672d [output] 42:a3:4b:f3:ca:05 vlan 0 permanent 02:42:0a:00:00:06 dst 159.89.39.105 self permanent Where, 02:42:0a:00:00:06 should be the mac id of the container on the other hosts e.g. on swarm-2 root@swarm-02:~# docker exec 92ee739ecdca ip lin 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 16: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1424 qdisc noqueue state UP link/ether 02:42:0a:00:00:06 brd ff:ff:ff:ff:ff:ff root@swarm-02:~# ip link 17: veth353ea84: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1424 qdisc noqueue master ov-001001-7672d state UP mode DEFAULT group default link/ether 6e:2a:bb:f1:52:2a brd ff:ff:ff:ff:ff:ff Task: Do the same from the other end. Check the fdb for vxlan interface on swarm-02 and correlate it with the mac id of a container running on swarm-01","title":"Launch Service with overlay network"},{"location":"swarm-networking-deepdive/#launch-worker-service-in-same-overlay","text":"docker service create --name worker --network vote schoolofdevops/worker Should get launched on the third node as the default scheduling algorithm is to sread the load evenly. Now on node1 and node2 check the fdb again, should see a new vtep endpoint e.g. [replace vx-001001-7672d with the id of the vxlan interface created for this overlayn/w, get it by using brctl show ] bridge fdb show dev vx-001001-7672d 22:ba:86:43:71:27 vlan 0 permanent 02:42:0a:00:00:07 dst 159.65.161.208 self permanent 02:42:0a:00:00:09 dst 165.227.64.215 self permanent","title":"Launch worker service in same overlay"},{"location":"swarm-networking-deepdive/#scale-redis-service","text":"docker service scale redis=5 Examine the fdb again bridge fdb show dev vx-001001-7672d 42:a3:4b:f3:ca:05 vlan 0 permanent 02:42:0a:00:00:09 vlan 0 02:42:0a:00:00:06 dst 159.89.39.105 self permanent 02:42:0a:00:00:09 dst 165.227.64.215 self permanent 02:42:0a:00:00:0a dst 165.227.64.215 self permanent 02:42:0a:00:00:0b dst 159.89.39.105 self permanent where, the table shows entries for every other","title":"Scale redis service"},{"location":"swarm-networking-deepdive/#underlying-vxlan-service-and-traffic","text":"port 4789 is reservered for vxlan. Packets will have headers with this. netstat -pan | grep 4789 To see the packets going through the vxlan interface brctl show tcpdump -i ov-001001-7672d","title":"Underlying VXLAN service  and traffic"},{"location":"swarm-networking-deepdive/#internal-load-balancing","text":"connect to one of the redis instances on one of the nodes docker ps docker run --rm -it --net container:0b0309771045 --privileged nicolaka/netshoot Verify redirect to ipvs iptables -nvL -t nat Chain POSTROUTING (policy ACCEPT 85 packets, 5426 bytes) pkts bytes target prot opt in out source destination 59 3712 DOCKER_POSTROUTING all -- * * 0.0.0.0/0 127.0.0.11 3 180 SNAT all -- * * 0.0.0.0/0 10.0.0.0/24 ipvs to:10.0.0.11 where, ipvs to:10.0.0.11 . : is routing the traffic to ipvs, running on the same container check the mangle markers iptables -nvL -t mangle Chain OUTPUT (policy ACCEPT 864 packets, 62611 bytes) pkts bytes target prot opt in out source destination 0 0 MARK all -- * * 0.0.0.0/0 10.0.0.5 MARK set 0x100 168 14112 MARK all -- * * 0.0.0.0/0 10.0.0.8 MARK set 0x101 26 1757 MARK all -- * * 0.0.0.0/0 10.0.0.13 MARK set 0x103 where, 10.0.0.5 is s VIP for service xyz . 0x100is a HEX for 256. to check where this is redirecting, look at the ipvs rules # ipvsadm IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -> RemoteAddress:Port Forward Weight ActiveConn InActConn FWM 256 rr -> redis.1.m911y2nt3wy0qo5x8i9p Masq 1 0 0 -> redis.2.pbyb0o2e60gm1ozwc3wz Masq 1 0 0 -> e89fbe75fb1a.vote:0 Masq 1 0 0 -> 0b0309771045:0 Masq 1 0 0 -> redis.5.oag34plamnmptoby9b0y Masq 1 0 0 FWM 257 rr -> worker.1.lhxvgjgn5k6soaksifz Masq 1 0 0 FWM 259 rr -> vote.1.hhv9l10vb5yocxmkbzzdv Masq 1 0 0 -> vote.2.h1iwvk5t8hr9pc6mpqsxk Masq 1 0 0 here the following is doing a RR load balancing across 5 nodes FWM 256 rr -> redis.1.m911y2nt3wy0qo5x8i9p Masq 1 0 0 -> redis.2.pbyb0o2e60gm1ozwc3wz Masq 1 0 0 -> e89fbe75fb1a.vote:0 Masq 1 0 0 -> 0b0309771045:0 Masq 1 0 0 -> redis.5.oag34plamnmptoby9b0y Masq 1 0 0","title":"Internal Load Balancing"},{"location":"swarm-networking-deepdive/#port-publishing-routing-mesh-ingress-network-and-external-service-discovery","text":"docker network ls docker network inspect ingress where, Peers : shows all the hosts which are part of this ingress (note the peers and corraborate) Containers : shows ingress-sbox namespace (its not a containers, just a namespace, has one interface in gwbridge, another ingress)","title":"Port Publishing, Routing Mesh, Ingress Network and External Service Discovery"},{"location":"swarm-networking-deepdive/#examine-the-ingress-sbox-namespace","text":"Learn about netshoot utility at https://github.com/nicolaka/netshoot Launch netshoot container, and connect to ingress-sbox using nsenter. docker run -it --rm -v /var/run/docker/netns:/netns --privileged=true nicolaka/netshoot nsenter --net=/netns/ingress_sbox sh From inside netshoot container, ``` ifconfig ip link show [output] 1: lo: mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 9: eth0: mtu 1450 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:0a:ff:00:02 brd ff:ff:ff:ff:ff:ff 12: eth1: mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff ``` On the host ip link show [output] 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 92:20:8a:88:b6:e8 brd ff:ff:ff:ff:ff:ff 3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:68:98:7c:d3 brd ff:ff:ff:ff:ff:ff 7: ov-001000-lpq3t: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP mode DEFAULT group default link/ether 8a:17:46:93:46:94 brd ff:ff:ff:ff:ff:ff 8: vx-001000-lpq3t: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master ov-001000-lpq3t state UNKNOWN mode DEFAULT group default link/ether 8a:17:46:93:46:94 brd ff:ff:ff:ff:ff:ff 10: veth28c87ba: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master ov-001000-lpq3t state UP mode DEFAULT group default link/ether 8a:24:17:29:46:a7 brd ff:ff:ff:ff:ff:ff 11: docker_gwbridge: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:a0:ca:1d:96 brd ff:ff:ff:ff:ff:ff 13: veth0740008: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker_gwbridge state UP mode DEFAULT group default link/ether aa:b8:35:c2:97:74 brd ff:ff:ff:ff:ff:ff 19: veth97a403d: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default link/ether 8a:80:a4:17:3b:2d brd ff:ff:ff:ff:ff:ff If you compare two outputs above, 9 <=====> 10 : ingress network 12 <=====> 13 : docker_gwbridge network These are then further bridged. Examine the bridges in the next part. Create a container which is part of this ingress docker service create --name vote --network vote --publish 80 --replicas=2 schoolofdevops/vote docker ps [output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 85642d7fa2f4 schoolofdevops/vote:latest \"gunicorn app:app -b\ufffd\" 28 seconds ago Up 27 seconds 80/tcp vote.1.hhv9l10vb5yocxmkbzzdvtmj2 64f05b29e559 redis:alpine \"docker-entrypoint.s\ufffd\" 29 minutes ago Up 29 minutes 6379/tcp redis.5.oag34plamnmptoby9b0yuaooi 4ea9c75179c3 redis:alpine \"docker-entrypoint.s\ufffd\" About an hour ago Up About an hour 6379/tcp redis.2.pbyb0o2e60gm1ozwc3wz9f7ou Connect to container and examine docker exec 85642d7fa2f4 ifconfig docker exec 85642d7fa2f4 ip link show docker exec 85642d7fa2f4 netstat -nr Correlate it with the host veth pair ip link show brctl show docker network ls eth0 => ingress eth1 => gwbridge eth2 => overlay for apps","title":"Examine the ingress-sbox namespace"},{"location":"swarm-networking-deepdive/#service-networking-and-routing-mesh","text":"For external facing ingress connnetiion, service routing works this way, ingress ==> gwbridge ==> ingress-sbox (its just a n/w namespae not a container) ==> ipvs ==> underlay Check iptable rules on the host iptables -nvL -t nat [output] ... Chain DOCKER-INGRESS (2 references) pkts bytes target prot opt in out source destination 0 0 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:30000 to:172.18.0.2:30000 31 1744 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 where, tcp dpt:30000 to:172.18.0.2:30000 => is forwarding the traffic received on 30000 port to 172.18.0.2:30000. Here 172.18.0.2 belongs to ingress_sbox so whatever happens next is inside there... Connecting to ingress-sbox docker run -it --rm -v /var/run/docker/netns:/var/run/docker/netns --privileged=true nicolaka/netshoot nsenter --net=/var/run/docker/netns/ingress_sbox sh alternately docker run -it --rm -v /var/run/docker/netns:/netns --privileged=true nicolaka/netshoot nsenter --net=/netns/ingress_sbox sh and then iptables -nvL -t mangle Chain PREROUTING (policy ACCEPT 16 packets, 1888 bytes) pkts bytes target prot opt in out source destination 0 0 MARK tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:30000 MARK set 0x102 where, iptables is setting MARK to 0x102 for anything that comes in on 30000 port. 0x102 is hex value and can be translated into integer from here https://www.binaryhexconverter.com/hex-to-decimal-converter e.g. 0x102 = 258 Now check the rules for above mark with ipvs ipvsadm [output] IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -> RemoteAddress:Port Forward Weight ActiveConn InActConn FWM 258 rr -> 10.255.0.6:0 Masq 1 0 0 -> 10.255.0.7:0 Masq 1 0 0 This is where the decision is made as to where this packet goes. Since ipvs uses round robin algorithm, one of these ips are selected and then packet is sent over the ingress overlay network. Finally, To see the traffic on ingress network on node2 tcpdump -i eth0 udp and port 4789 tcpdump -i eth0 esp","title":"Service Networking and Routing Mesh"},{"location":"swarm-networking-deepdive/#additional-commands","text":"tail -f syslog tcpdump -i eth0 udp and port 4789 tcpdump -i eth0 esp ip addr ip link iptables -t nat -nvL To see namespaces on the docker host cd /var/run ln -s /var/run/docker/netns netns ip netns docker network ls [company network and ns ids] References CNM and Libnetwork https://github.com/docker/libnetwork/blob/master/docs/design.md How VXLANs work ? https://youtu.be/Jqm_4TMmQz8?t=32s (watch from 00.32 to xx.xx) https://www.youtube.com/watch?v=YNqKDI_bnPM Overlay Tutorial https://neuvector.com/network-security/docker-swarm-container-networking/ Docker Networking Tutorial - Learning by Practicing https://www.securitynik.com/2016/12/docker-networking-internals-container.html Swarm networks https://docs.docker.com/v17.09/engine/swarm/networking/ Ip cheatsheet https://access.redhat.com/sites/default/files/attachments/rh_ip_command_cheatsheet_1214_jcs_print.pdf Overlay issues https://github.com/moby/moby/issues/30820 Network Troubleshooting https://success.docker.com/article/troubleshooting-container-networking Connect Service to Multiple Networks: https://www.slideshare.net/SreenivasMakam/docker-networking-common-issues-and-troubleshooting-techniques","title":"Additional Commands"},{"location":"swarm-quickdive/","text":"Lab: Docker SWARM Quick Dive Create a 5 nodes (3 masters, 2nodes) swarm cluster using http://play-with-docker.com Launch a Visualizer on Master (SWARM Manager) docker run -itd -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock schoolofdevops/visualizer Deploying Service with swarm - The imperative way docker service create --name vote schoolofdevops/vote docker service ls docker service inspect docker service update --publish-add 80:80 vote Try accessing port 80 on any of the nodes in the swarm cluster to validate. Scaling a service docker service scale vote=4 docker service ls docker service scale vote=2 Cleaning Up docker service rm vote Orchestrating Applications with Stack Deploy file: stack.yml version: \"3\" networks: nw01: driver: overlay volumes: db-data: services: vote: image: schoolofdevops/vote:v1 ports: - 80 networks: - nw01 depends_on: - redis deploy: replicas: 8 update_config: parallelism: 2 delay: 20s restart_policy: condition: on-failure redis: image: redis:alpine networks: - nw01 worker: image: schoolofdevops/vote-worker networks: - nw01 depends_on: - redis - db db: image: postgres:9.4 networks: - nw01 volumes: - db-data:/var/lib/postgresql/data result: image: schoolofdevops/vote-result ports: - 5001:80 networks: - nw01 depends_on: - db You could also copy the above file using the followinng command, wget -chttps://gist.githubusercontent.com/initcron/8a5ebd534df74ab2a83e96218b56137d/raw/9e748637aed121b67ceddeca8678750596c81ab7/stack.yml Deploy a stack docker stack deploy --compose-file stack.yml instavote Validate docker stack ls docker stack services instavote docker service ls docker service scale instavote_vote=4 Deploying a new version Update stack.yml with the new version of the image .... services: vote: image: schoolofdevops/vote:v2 ..... deploy: replicas: 8 update_config: parallelism: 2 delay: 20s restart_policy: condition: on-failure ... Deploy using the same command as earlier, docker stack deploy --compose-file stack.yml instavote Fault Tolerance Delete a node Observe the node being removed from cluster Observe tasks getting rescheduled automatically on available nodes","title":"Lab: Docker SWARM Quick Dive"},{"location":"swarm-quickdive/#lab-docker-swarm-quick-dive","text":"Create a 5 nodes (3 masters, 2nodes) swarm cluster using http://play-with-docker.com","title":"Lab: Docker SWARM Quick Dive"},{"location":"swarm-quickdive/#launch-a-visualizer-on-master-swarm-manager","text":"docker run -itd -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock schoolofdevops/visualizer","title":"Launch a Visualizer on Master (SWARM Manager)"},{"location":"swarm-quickdive/#deploying-service-with-swarm-the-imperative-way","text":"docker service create --name vote schoolofdevops/vote docker service ls docker service inspect docker service update --publish-add 80:80 vote Try accessing port 80 on any of the nodes in the swarm cluster to validate.","title":"Deploying Service with swarm - The imperative way"},{"location":"swarm-quickdive/#scaling-a-service","text":"docker service scale vote=4 docker service ls docker service scale vote=2","title":"Scaling a service"},{"location":"swarm-quickdive/#cleaning-up","text":"docker service rm vote","title":"Cleaning Up"},{"location":"swarm-quickdive/#orchestrating-applications-with-stack-deploy","text":"file: stack.yml version: \"3\" networks: nw01: driver: overlay volumes: db-data: services: vote: image: schoolofdevops/vote:v1 ports: - 80 networks: - nw01 depends_on: - redis deploy: replicas: 8 update_config: parallelism: 2 delay: 20s restart_policy: condition: on-failure redis: image: redis:alpine networks: - nw01 worker: image: schoolofdevops/vote-worker networks: - nw01 depends_on: - redis - db db: image: postgres:9.4 networks: - nw01 volumes: - db-data:/var/lib/postgresql/data result: image: schoolofdevops/vote-result ports: - 5001:80 networks: - nw01 depends_on: - db You could also copy the above file using the followinng command, wget -chttps://gist.githubusercontent.com/initcron/8a5ebd534df74ab2a83e96218b56137d/raw/9e748637aed121b67ceddeca8678750596c81ab7/stack.yml Deploy a stack docker stack deploy --compose-file stack.yml instavote Validate docker stack ls docker stack services instavote docker service ls docker service scale instavote_vote=4","title":"Orchestrating Applications with Stack Deploy"},{"location":"swarm-quickdive/#deploying-a-new-version","text":"Update stack.yml with the new version of the image .... services: vote: image: schoolofdevops/vote:v2 ..... deploy: replicas: 8 update_config: parallelism: 2 delay: 20s restart_policy: condition: on-failure ... Deploy using the same command as earlier, docker stack deploy --compose-file stack.yml instavote","title":"Deploying a new version"},{"location":"swarm-quickdive/#fault-tolerance","text":"Delete a node Observe the node being removed from cluster Observe tasks getting rescheduled automatically on available nodes","title":"Fault Tolerance"},{"location":"troubleshooting-toolkit/","text":"Troubleshooting Toolkit Netshoot Learn about netshoot, a swiff army knife troubleshooting utility here https://github.com/nicolaka/netshoot docker pull nicolaka/netshoot docker container run --name trsh-01 -idt debian bash docker exec -it trsh-01 bash try running some networking commands ifconfig ipvsadm netstat Connect to another container's network with netshoot docker run -it --net container:trsh-01 --privileged nicolaka/netshoot ifconfig ipvsadm netstat Connect to host namespace docker run -it --net host --privileged nicolaka/netshoot Connect to a network namespace using netshoot cd /var/run sudo ln -s /var/run/docker/netns netns sudo ip netns [output] f340b46b5428 default 6ce0f3206bb8 (id: 0) Lets enter the namespace default using netshoot docker run -it --rm -v /var/run/docker/netns:/netns --privileged=true nicolaka/netshoot nsenter --net=/netns/default sh Try the following with netshoot iperf : networking performance between containers/hosts tcpdump : packet capture and analysis netstat : network configurations, port to pid mapping, connections nmap : port scanning iftop : network interface top drill : name resolution, dns debugging ip route : Network commmands to remember docker network <commands> nsenter \u2014net=<net-namespace> tcpdump -nnvvXXS -i <interface> port <port> iptables -nvL -t <table> ipvsadm -L ip <commands> bridge <commands> drill netstat -tulpn iperf <commands> Finding ip routes and ARP neighbours ip route show [replace 172.17.0.4 with the ip address of a actual neighbour and docker0 with the interface ] ip neigh show ip neigh delete 172.17.0.4 dev docker0 ip neigh show ping -c 1 172.17.0.4 ip neigh show Ref : http://lartc.org/howto/lartc.iproute2.arp.html","title":"Troubleshooting Toolkit"},{"location":"troubleshooting-toolkit/#troubleshooting-toolkit","text":"","title":"Troubleshooting Toolkit"},{"location":"troubleshooting-toolkit/#netshoot","text":"Learn about netshoot, a swiff army knife troubleshooting utility here https://github.com/nicolaka/netshoot docker pull nicolaka/netshoot docker container run --name trsh-01 -idt debian bash docker exec -it trsh-01 bash try running some networking commands ifconfig ipvsadm netstat Connect to another container's network with netshoot docker run -it --net container:trsh-01 --privileged nicolaka/netshoot ifconfig ipvsadm netstat Connect to host namespace docker run -it --net host --privileged nicolaka/netshoot Connect to a network namespace using netshoot cd /var/run sudo ln -s /var/run/docker/netns netns sudo ip netns [output] f340b46b5428 default 6ce0f3206bb8 (id: 0) Lets enter the namespace default using netshoot docker run -it --rm -v /var/run/docker/netns:/netns --privileged=true nicolaka/netshoot nsenter --net=/netns/default sh Try the following with netshoot iperf : networking performance between containers/hosts tcpdump : packet capture and analysis netstat : network configurations, port to pid mapping, connections nmap : port scanning iftop : network interface top drill : name resolution, dns debugging ip route : Network commmands to remember docker network <commands> nsenter \u2014net=<net-namespace> tcpdump -nnvvXXS -i <interface> port <port> iptables -nvL -t <table> ipvsadm -L ip <commands> bridge <commands> drill netstat -tulpn iperf <commands>","title":"Netshoot"},{"location":"troubleshooting-toolkit/#finding-ip-routes-and-arp-neighbours","text":"ip route show [replace 172.17.0.4 with the ip address of a actual neighbour and docker0 with the interface ] ip neigh show ip neigh delete 172.17.0.4 dev docker0 ip neigh show ping -c 1 172.17.0.4 ip neigh show Ref : http://lartc.org/howto/lartc.iproute2.arp.html","title":"Finding ip routes and ARP neighbours"}]}